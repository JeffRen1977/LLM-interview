#!/usr/bin/env python3
"""
Memory-Efficient Training Implementation

This implementation demonstrates memory-efficient training techniques including
mixed precision training and gradient checkpointing, extracted from 
openAI_memory_efficient_training.md.

Author: Extracted from openAI_memory_efficient_training.md
"""

import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint  # ÂØºÂÖ•Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ
from torch.cuda.amp import GradScaler, autocast  # ÂØºÂÖ•Ê∑∑ÂêàÁ≤æÂ∫¶Â∑•ÂÖ∑
import time


# --- 1. ÂÆö‰πâ‰∏Ä‰∏™Ê®°ÊãüÁöÑÂ§ßÂûãÊ®°Âûã ---
# ÂåÖÂê´Â§ö‰∏™ÂÜÖÂ≠òÊ∂àËÄóÂ§ßÁöÑ Block
class MemoryIntensiveBlock(nn.Module):
    def __init__(self, hidden_dim):
        super().__init__()
        self.linear1 = nn.Linear(hidden_dim, 4 * hidden_dim)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(4 * hidden_dim, hidden_dim)
    
    def forward(self, x):
        return self.linear2(self.relu(self.linear1(x)))


class LargeModel(nn.Module):
    def __init__(self, num_layers=32, hidden_dim=1024, use_checkpointing=False):
        super().__init__()
        self.use_checkpointing = use_checkpointing
        self.layers = nn.ModuleList(
            [MemoryIntensiveBlock(hidden_dim) for _ in range(num_layers)]
        )
    
    def forward(self, x):
        for layer in self.layers:
            if self.use_checkpointing and self.training:
                # Âè™Âú®ËÆ≠ÁªÉÊó∂‰ΩøÁî® checkpoint
                x = checkpoint(layer, x)
            else:
                x = layer(x)
        return x


# ËæÖÂä©ÂáΩÊï∞ÔºöËøêË°å‰∏Ä‰∏™ËÆ≠ÁªÉÊ≠•È™§Âπ∂Êä•ÂëäÂ≥∞ÂÄºÂÜÖÂ≠ò
def run_training_step(model, optimizer, use_amp=False):
    scaler = GradScaler() if use_amp else None
    
    # Ê®°ÊãüËæìÂÖ•Êï∞ÊçÆ
    input_data = torch.randn(16, 1024, 1024).cuda()  # (batch, seq_len, hidden)
    model.train()
    optimizer.zero_grad()
    
    # --- Ê†∏ÂøÉÔºöÊ∑∑ÂêàÁ≤æÂ∫¶ ---
    # autocast ‰ºöËá™Âä®Â∞ÜÊìç‰ΩúËΩ¨‰∏∫ FP16
    with autocast(enabled=use_amp):
        output = model(input_data)
        loss = output.mean()

    if use_amp:
        scaler.scale(loss).backward()
        scaler.step(optimizer)
        scaler.update()
    else:  # Ê†áÂáÜ FP32 ËÆ≠ÁªÉ
        loss.backward()
        optimizer.step()
        
    peak_memory_gb = torch.cuda.max_memory_allocated() / (1024**3)
    torch.cuda.reset_peak_memory_stats()  # ÈáçÁΩÆÁªüËÆ°
    return peak_memory_gb


def compare_memory_usage():
    """Compare memory usage across different optimization techniques"""
    
    print("üöÄ Memory-Efficient Training Comparison")
    print("=" * 60)
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        print("‚ùå ÈúÄË¶Å CUDA GPU Êù•ËøêË°åÊ≠§Á§∫‰æã„ÄÇ")
        return
    
    results = {}
    
    # Âú∫ÊôØ 1: Ê†áÂáÜËÆ≠ÁªÉ (ÂèØËÉΩ‰ºö OOM)
    print("--- Âú∫ÊôØ 1: Ê†áÂáÜ FP32 ËÆ≠ÁªÉ ---")
    model_base = LargeModel(use_checkpointing=False).to(device)
    optimizer_base = torch.optim.Adam(model_base.parameters())
    try:
        mem_base = run_training_step(model_base, optimizer_base, use_amp=False)
        results['Standard FP32'] = mem_base
        print(f"‚úÖ Â≥∞ÂÄºÂÜÖÂ≠òÂç†Áî®: {mem_base:.2f} GB")
    except RuntimeError as e:
        results['Standard FP32'] = "OOM"
        print(f"‚ùå ÂÜÖÂ≠ò‰∏çË∂≥ (OOM): {e}")
    del model_base, optimizer_base
    torch.cuda.empty_cache()

    # Âú∫ÊôØ 2: ‰ªÖ‰ΩøÁî®Ê∑∑ÂêàÁ≤æÂ∫¶
    print("\n--- Âú∫ÊôØ 2: ‰ΩøÁî®Ê∑∑ÂêàÁ≤æÂ∫¶ (AMP) ---")
    model_amp = LargeModel(use_checkpointing=False).to(device)
    optimizer_amp = torch.optim.Adam(model_amp.parameters())
    mem_amp = run_training_step(model_amp, optimizer_amp, use_amp=True)
    results['Mixed Precision'] = mem_amp
    print(f"‚úÖ Â≥∞ÂÄºÂÜÖÂ≠òÂç†Áî®: {mem_amp:.2f} GB")
    del model_amp, optimizer_amp
    torch.cuda.empty_cache()

    # Âú∫ÊôØ 3: ‰ªÖ‰ΩøÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ
    print("\n--- Âú∫ÊôØ 3: ‰ΩøÁî®Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ ---")
    model_cp = LargeModel(use_checkpointing=True).to(device)
    optimizer_cp = torch.optim.Adam(model_cp.parameters())
    mem_cp = run_training_step(model_cp, optimizer_cp, use_amp=False)
    results['Gradient Checkpointing'] = mem_cp
    print(f"‚úÖ Â≥∞ÂÄºÂÜÖÂ≠òÂç†Áî®: {mem_cp:.2f} GB")
    del model_cp, optimizer_cp
    torch.cuda.empty_cache()

    # Âú∫ÊôØ 4: ÁªìÂêà‰∏§ËÄÖ
    print("\n--- Âú∫ÊôØ 4: Ê∑∑ÂêàÁ≤æÂ∫¶ + Ê¢ØÂ∫¶Ê£ÄÊü•ÁÇπ ---")
    model_both = LargeModel(use_checkpointing=True).to(device)
    optimizer_both = torch.optim.Adam(model_both.parameters())
    mem_both = run_training_step(model_both, optimizer_both, use_amp=True)
    results['Combined'] = mem_both
    print(f"‚úÖ Â≥∞ÂÄºÂÜÖÂ≠òÂç†Áî®: {mem_both:.2f} GB")
    del model_both, optimizer_both
    torch.cuda.empty_cache()

    # Print summary
    print("\n" + "=" * 60)
    print("üìä Memory Usage Summary")
    print("=" * 60)
    for technique, memory in results.items():
        if memory == "OOM":
            print(f"{technique:25} | OOM (Out of Memory)")
        else:
            print(f"{technique:25} | {memory:.2f} GB")
    
    return results


def benchmark_performance():
    """Benchmark training speed with different techniques"""
    
    device = "cuda" if torch.cuda.is_available() else "cpu"
    if device == "cpu":
        print("‚ùå CUDA GPU required for performance benchmarking.")
        return
    
    print("\nüèÉ Performance Benchmarking")
    print("=" * 60)
    
    num_iterations = 10
    results = {}
    
    # Test different configurations
    configs = [
        ("Standard FP32", False, False),
        ("Mixed Precision", True, False),
        ("Gradient Checkpointing", False, True),
        ("Combined", True, True),
    ]
    
    for config_name, use_amp, use_checkpointing in configs:
        print(f"\nTesting {config_name}...")
        
        model = LargeModel(use_checkpointing=use_checkpointing).to(device)
        optimizer = torch.optim.Adam(model.parameters())
        
        # Warmup
        for _ in range(3):
            run_training_step(model, optimizer, use_amp=use_amp)
        
        # Benchmark
        start_time = time.time()
        for _ in range(num_iterations):
            run_training_step(model, optimizer, use_amp=use_amp)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / num_iterations
        results[config_name] = avg_time
        
        print(f"  Average time per iteration: {avg_time:.4f}s")
        
        del model, optimizer
        torch.cuda.empty_cache()
    
    # Print performance summary
    print("\n" + "=" * 60)
    print("‚è±Ô∏è  Performance Summary")
    print("=" * 60)
    baseline_time = results.get("Standard FP32", 1.0)
    
    for technique, time_taken in results.items():
        speedup = baseline_time / time_taken if time_taken > 0 else 0
        print(f"{technique:25} | {time_taken:.4f}s | {speedup:.2f}x speedup")
    
    return results


def demonstrate_techniques():
    """Demonstrate the memory-efficient training techniques"""
    
    print("üß† Memory-Efficient Training Techniques Demo")
    print("Extracted from openAI_memory_efficient_training.md")
    print("=" * 60)
    
    # Check CUDA availability
    if torch.cuda.is_available():
        print(f"‚úÖ CUDA available: {torch.cuda.get_device_name()}")
        print(f"   Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        print("‚ö†Ô∏è  CUDA not available. Some features may not work.")
    
    # Run memory comparison
    memory_results = compare_memory_usage()
    
    # Run performance benchmark
    performance_results = benchmark_performance()
    
    print("\n" + "=" * 60)
    print("üéâ All demonstrations completed!")
    print("=" * 60)


def main():
    """Main function to run the memory-efficient training demonstration"""
    
    print("üíæ Memory-Efficient Training Implementation")
    print("This demonstrates mixed precision training and gradient checkpointing")
    print("as described in openAI_memory_efficient_training.md")
    print("=" * 60)
    
    try:
        demonstrate_techniques()
    except Exception as e:
        print(f"\n‚ùå Error during demonstration: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()