"""
OpenAI è¨“ç·´ä¸ç©©å®šæ€§å•é¡Œèª¿è©¦å·¥å…·

æœ¬æ¨¡çµ„æä¾›äº†ä¸€å¥—å®Œæ•´çš„è¨“ç·´ä¸ç©©å®šæ€§è¨ºæ–·å’Œè§£æ±ºæ–¹æ¡ˆï¼Œå°ˆé–€é‡å°æ·±åº¦å­¸ç¿’æ¨¡å‹
è¨“ç·´éç¨‹ä¸­å¯èƒ½é‡åˆ°çš„å„ç¨®ç©©å®šæ€§å•é¡Œã€‚

ä¸»è¦åŠŸèƒ½:
1. è¨“ç·´ç©©å®šæ€§è¨ºæ–·
   - æ¢¯åº¦èŒƒæ•¸ç›£æ§å’Œåˆ†æ
   - æ¬Šé‡èŒƒæ•¸çµ±è¨ˆå’Œæª¢æŸ¥
   - æ¿€æ´»å€¼åˆ†å¸ƒåˆ†æ
   - æå¤±å‡½æ•¸ç•°å¸¸æª¢æ¸¬

2. ç©©å®šè¨“ç·´å™¨å¯¦ç¾
   - æ¢¯åº¦è£å‰ªå’Œæ­£å‰‡åŒ–
   - å­¸ç¿’ç‡é ç†±å’Œèª¿åº¦
   - æ•¸å€¼ç©©å®šæ€§ä¿è­·
   - æ¨¡å‹å¥åº·ç‹€æ…‹ç›£æ§

3. å¸¸è¦‹å•é¡Œä¿®å¾©
   - å­¸ç¿’ç‡èª¿æ•´ç­–ç•¥
   - æ¢¯åº¦å•é¡Œè§£æ±ºæ–¹æ¡ˆ
   - æ•¸æ“šè³ªé‡æª¢æŸ¥
   - æ¨¡å‹æ¶æ§‹å„ªåŒ–

4. è¨ºæ–·å·¥å…·å’Œå¯è¦–åŒ–
   - å¯¦æ™‚ç›£æ§æŒ‡æ¨™
   - çµ±è¨ˆåˆ†æå’Œå ±å‘Š
   - å•é¡Œè­˜åˆ¥å’Œå»ºè­°
   - æ€§èƒ½åŸºæº–æ¸¬è©¦

æŠ€è¡“ç‰¹é»:
- æ”¯æŒå¤šç¨®å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦ç­–ç•¥
- æä¾›è©³ç´°çš„æ¢¯åº¦åˆ†æå·¥å…·
- åŒ…å«æ•¸å€¼ç©©å®šæ€§ä¿è­·æ©Ÿåˆ¶
- æ”¯æŒæ··åˆç²¾åº¦è¨“ç·´
- æä¾›å®Œæ•´çš„éŒ¯èª¤è™•ç†å’Œæ—¥èªŒè¨˜éŒ„

é©ç”¨å ´æ™¯:
- æ·±åº¦å­¸ç¿’æ¨¡å‹è¨“ç·´èª¿è©¦
- è¨“ç·´ä¸ç©©å®šæ€§å•é¡Œè¨ºæ–·
- æ¨¡å‹æ€§èƒ½å„ªåŒ–
- ç”Ÿç”¢ç’°å¢ƒç©©å®šæ€§ç›£æ§

ä½œè€…: OpenAI Interview Preparation
ç‰ˆæœ¬: 1.0.0
æ›´æ–°æ—¥æœŸ: 2024
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
from torch.utils.data import DataLoader, TensorDataset
from collections import defaultdict
import warnings
import logging
from typing import Dict, List, Tuple

# é…ç½®æ—¥èªŒç³»çµ±
# è¨­ç½®æ—¥èªŒç´šåˆ¥ç‚ºINFOï¼Œç¢ºä¿é‡è¦ä¿¡æ¯è¢«è¨˜éŒ„
# ä½¿ç”¨æ¨™æº–çš„Python loggingæ¨¡çµ„é€²è¡Œæ—¥èªŒç®¡ç†
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class TrainingStabilityDiagnostics:
    """
    è¨“ç·´ç©©å®šæ€§è¨ºæ–·å·¥å…·é¡
    
    é€™å€‹é¡æä¾›äº†ä¸€å¥—å®Œæ•´çš„è¨“ç·´ç©©å®šæ€§è¨ºæ–·å·¥å…·ï¼Œç”¨æ–¼ç›£æ§å’Œåˆ†ææ·±åº¦å­¸ç¿’æ¨¡å‹
    è¨“ç·´éç¨‹ä¸­çš„å„ç¨®ç©©å®šæ€§æŒ‡æ¨™ã€‚å®ƒèƒ½å¤ æª¢æ¸¬æ¢¯åº¦çˆ†ç‚¸ã€æ¢¯åº¦æ¶ˆå¤±ã€æ¬Šé‡ç•°å¸¸ã€
    æ¿€æ´»å€¼å•é¡Œç­‰å¸¸è¦‹çš„è¨“ç·´ä¸ç©©å®šæ€§å•é¡Œã€‚
    
    ä¸»è¦åŠŸèƒ½:
    1. æ¢¯åº¦èŒƒæ•¸ç›£æ§
       - è¨ˆç®—ç¸½é«”æ¢¯åº¦èŒƒæ•¸
       - åˆ†æå„å±¤æ¢¯åº¦åˆ†å¸ƒ
       - æª¢æ¸¬æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±
       - æä¾›æ¢¯åº¦çµ±è¨ˆä¿¡æ¯
    
    2. æ¬Šé‡èŒƒæ•¸åˆ†æ
       - ç›£æ§æ¬Šé‡åƒæ•¸èŒƒæ•¸
       - åˆ†ææ¬Šé‡åˆ†å¸ƒçµ±è¨ˆ
       - æª¢æ¸¬æ¬Šé‡ç•°å¸¸è®ŠåŒ–
       - æä¾›æ¬Šé‡å¥åº·ç‹€æ…‹
    
    3. æ¿€æ´»å€¼æª¢æŸ¥
       - åˆ†æå„å±¤æ¿€æ´»å€¼åˆ†å¸ƒ
       - æª¢æ¸¬NaNå’ŒInfå€¼
       - ç›£æ§æ¿€æ´»å€¼ç¯„åœ
       - æä¾›æ¿€æ´»çµ±è¨ˆä¿¡æ¯
    
    4. æå¤±å‡½æ•¸è¨ºæ–·
       - æª¢æ¸¬æå¤±è·³èº
       - è­˜åˆ¥æŒ¯ç›ªæ¨¡å¼
       - ç™¼ç¾ç•°å¸¸å€¼
       - æä¾›ç©©å®šæ€§å»ºè­°
    
    æŠ€è¡“ç‰¹é»:
    - å¯¦æ™‚ç›£æ§å’Œè¨ºæ–·
    - è©³ç´°çš„çµ±è¨ˆåˆ†æ
    - è‡ªå‹•å•é¡Œæª¢æ¸¬
    - å¯è¦–åŒ–æ”¯æŒ
    - ç”Ÿç”¢ç’°å¢ƒå‹å¥½
    
    ä½¿ç”¨ç¤ºä¾‹:
        diagnostics = TrainingStabilityDiagnostics()
        
        # è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
        diagnostics.log_metrics(epoch=0, loss=0.5, accuracy=0.8)
        
        # æª¢æŸ¥æ¢¯åº¦èŒƒæ•¸
        grad_stats = diagnostics.check_gradient_norms(model)
        
        # æª¢æŸ¥æ¬Šé‡èŒƒæ•¸
        weight_stats = diagnostics.check_weight_norms(model)
        
        # æª¢æŸ¥æ¿€æ´»å€¼
        activation_stats = diagnostics.check_activations(model, sample_input)
        
        # è¨ºæ–·ä¸ç©©å®šæ€§
        issues = diagnostics.diagnose_instability(loss_history)
    """
    
    def __init__(self):
        """
        åˆå§‹åŒ–è¨“ç·´ç©©å®šæ€§è¨ºæ–·å·¥å…·
        
        è¨­ç½®ç”¨æ–¼å­˜å„²å„ç¨®è¨ºæ–·æŒ‡æ¨™çš„æ•¸æ“šçµæ§‹ï¼ŒåŒ…æ‹¬:
        - metrics: å­˜å„²è¨“ç·´éç¨‹ä¸­çš„å„ç¨®æŒ‡æ¨™
        - gradient_norms: å­˜å„²æ¢¯åº¦èŒƒæ•¸æ­·å²
        - weight_norms: å­˜å„²æ¬Šé‡èŒƒæ•¸æ­·å²
        - activations: å­˜å„²æ¿€æ´»å€¼çµ±è¨ˆä¿¡æ¯
        """
        # ä½¿ç”¨defaultdictå­˜å„²å„ç¨®æŒ‡æ¨™ï¼Œè‡ªå‹•å‰µå»ºåˆ—è¡¨
        self.metrics = defaultdict(list)
        
        # å­˜å„²æ¢¯åº¦èŒƒæ•¸æ­·å²ï¼Œç”¨æ–¼åˆ†ææ¢¯åº¦è®ŠåŒ–è¶¨å‹¢
        self.gradient_norms = []
        
        # å­˜å„²æ¬Šé‡èŒƒæ•¸æ­·å²ï¼Œç”¨æ–¼ç›£æ§æ¬Šé‡è®ŠåŒ–
        self.weight_norms = []
        
        # å­˜å„²æ¿€æ´»å€¼çµ±è¨ˆä¿¡æ¯ï¼Œç”¨æ–¼åˆ†ææ¿€æ´»åˆ†å¸ƒ
        self.activations = []
        
    def log_metrics(self, epoch: int, loss: float, **kwargs):
        """
        è¨˜éŒ„è¨“ç·´æŒ‡æ¨™
        
        é€™å€‹æ–¹æ³•ç”¨æ–¼è¨˜éŒ„è¨“ç·´éç¨‹ä¸­çš„å„ç¨®æŒ‡æ¨™ï¼ŒåŒ…æ‹¬æå¤±å€¼ã€æº–ç¢ºç‡ã€
        å­¸ç¿’ç‡ç­‰ã€‚è¨˜éŒ„çš„æŒ‡æ¨™å¯ä»¥ç”¨æ–¼å¾ŒçºŒçš„ç©©å®šæ€§åˆ†æå’Œå¯è¦–åŒ–ã€‚
        
        åƒæ•¸:
            epoch (int): ç•¶å‰è¨“ç·´è¼ªæ¬¡
            loss (float): ç•¶å‰æå¤±å€¼
            **kwargs: å…¶ä»–éœ€è¦è¨˜éŒ„çš„æŒ‡æ¨™ï¼Œå¦‚accuracyã€learning_rateç­‰
        
        åŠŸèƒ½:
        - è‡ªå‹•è¨˜éŒ„epochå’Œloss
        - æ”¯æŒå‹•æ…‹æ·»åŠ å…¶ä»–æŒ‡æ¨™
        - ç¶­è­·æŒ‡æ¨™æ­·å²è¨˜éŒ„
        - æ”¯æŒå¾ŒçºŒåˆ†æå’Œå¯è¦–åŒ–
        
        ä½¿ç”¨ç¤ºä¾‹:
            diagnostics.log_metrics(epoch=0, loss=0.5, accuracy=0.8, lr=0.001)
        """
        # è¨˜éŒ„epochå’Œlossï¼Œé€™äº›æ˜¯åŸºæœ¬çš„è¨“ç·´æŒ‡æ¨™
        self.metrics['epoch'].append(epoch)
        self.metrics['loss'].append(loss)
        
        # è¨˜éŒ„å…¶ä»–å‹•æ…‹æŒ‡æ¨™
        for key, value in kwargs.items():
            self.metrics[key].append(value)
    
    def check_gradient_norms(self, model: nn.Module) -> Dict[str, float]:
        """
        æª¢æŸ¥æ¢¯åº¦èŒƒæ•¸
        
        é€™å€‹æ–¹æ³•è¨ˆç®—å’Œåˆ†ææ¨¡å‹ä¸­æ‰€æœ‰åƒæ•¸çš„æ¢¯åº¦èŒƒæ•¸ï¼Œç”¨æ–¼æª¢æ¸¬æ¢¯åº¦çˆ†ç‚¸
        å’Œæ¢¯åº¦æ¶ˆå¤±å•é¡Œã€‚æ¢¯åº¦èŒƒæ•¸æ˜¯è¡¡é‡æ¢¯åº¦å¤§å°çš„é‡è¦æŒ‡æ¨™ï¼Œå°æ–¼è¨“ç·´
        ç©©å®šæ€§è‡³é—œé‡è¦ã€‚
        
        åƒæ•¸:
            model (nn.Module): è¦æª¢æŸ¥çš„PyTorchæ¨¡å‹
        
        è¿”å›:
            Dict[str, float]: åŒ…å«æ¢¯åº¦èŒƒæ•¸çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
                - total_norm: ç¸½é«”æ¢¯åº¦èŒƒæ•¸ (L2èŒƒæ•¸)
                - max_norm: æœ€å¤§å–®å€‹åƒæ•¸æ¢¯åº¦èŒƒæ•¸
                - min_norm: æœ€å°å–®å€‹åƒæ•¸æ¢¯åº¦èŒƒæ•¸
                - avg_norm: å¹³å‡æ¢¯åº¦èŒƒæ•¸
                - layer_norms: å„å±¤æ¢¯åº¦èŒƒæ•¸è©³æƒ…
        
        æŠ€è¡“ç´°ç¯€:
        - ä½¿ç”¨L2èŒƒæ•¸è¨ˆç®—æ¢¯åº¦å¤§å°
        - åªè¨ˆç®—æœ‰æ¢¯åº¦çš„åƒæ•¸
        - æä¾›ç¸½é«”å’Œé€å±¤çš„çµ±è¨ˆä¿¡æ¯
        - æ”¯æŒæ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±æª¢æ¸¬
        
        ä½¿ç”¨ç¤ºä¾‹:
            grad_stats = diagnostics.check_gradient_norms(model)
            if grad_stats['total_norm'] > 10.0:
                print("è­¦å‘Š: æ¢¯åº¦èŒƒæ•¸éå¤§ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸")
        """
        # åˆå§‹åŒ–çµ±è¨ˆè®Šé‡
        total_norm = 0.0  # ç¸½é«”æ¢¯åº¦èŒƒæ•¸çš„å¹³æ–¹å’Œ
        param_count = 0   # æœ‰æ¢¯åº¦çš„åƒæ•¸æ•¸é‡
        max_norm = 0.0    # æœ€å¤§å–®å€‹åƒæ•¸æ¢¯åº¦èŒƒæ•¸
        min_norm = float('inf')  # æœ€å°å–®å€‹åƒæ•¸æ¢¯åº¦èŒƒæ•¸
        layer_norms = {}  # å„å±¤æ¢¯åº¦èŒƒæ•¸è©³æƒ…
        
        # éæ­·æ¨¡å‹ä¸­çš„æ‰€æœ‰åƒæ•¸
        for name, param in model.named_parameters():
            # åªè™•ç†æœ‰æ¢¯åº¦çš„åƒæ•¸
            if param.grad is not None:
                # è¨ˆç®—ç•¶å‰åƒæ•¸çš„L2èŒƒæ•¸
                param_norm = param.grad.data.norm(2)
                
                # è¨˜éŒ„å„å±¤çš„æ¢¯åº¦èŒƒæ•¸
                layer_norms[name] = param_norm.item()
                
                # ç´¯åŠ åˆ°ç¸½é«”èŒƒæ•¸çš„å¹³æ–¹å’Œä¸­
                total_norm += param_norm.item() ** 2
                param_count += 1
                
                # æ›´æ–°æœ€å¤§å€¼å’Œæœ€å°å€¼
                max_norm = max(max_norm, param_norm.item())
                min_norm = min(min_norm, param_norm.item())
        
        # è¨ˆç®—ç¸½é«”L2èŒƒæ•¸ (å¹³æ–¹å’Œçš„å¹³æ–¹æ ¹)
        total_norm = total_norm ** (1. / 2)
        
        # è¿”å›å®Œæ•´çš„æ¢¯åº¦çµ±è¨ˆä¿¡æ¯
        return {
            'total_norm': total_norm,  # ç¸½é«”æ¢¯åº¦èŒƒæ•¸
            'max_norm': max_norm,      # æœ€å¤§å–®å€‹åƒæ•¸æ¢¯åº¦èŒƒæ•¸
            'min_norm': min_norm if min_norm != float('inf') else 0.0,  # æœ€å°å–®å€‹åƒæ•¸æ¢¯åº¦èŒƒæ•¸
            'avg_norm': total_norm / max(param_count, 1),  # å¹³å‡æ¢¯åº¦èŒƒæ•¸
            'layer_norms': layer_norms  # å„å±¤æ¢¯åº¦èŒƒæ•¸è©³æƒ…
        }
    
    def check_weight_norms(self, model: nn.Module) -> Dict[str, float]:
        """
        æª¢æŸ¥æ¬Šé‡èŒƒæ•¸
        
        é€™å€‹æ–¹æ³•è¨ˆç®—å’Œåˆ†ææ¨¡å‹ä¸­æ‰€æœ‰æ¬Šé‡åƒæ•¸çš„èŒƒæ•¸å’Œçµ±è¨ˆä¿¡æ¯ï¼Œç”¨æ–¼
        ç›£æ§æ¬Šé‡çš„å¥åº·ç‹€æ…‹å’Œæª¢æ¸¬æ¬Šé‡ç•°å¸¸ã€‚æ¬Šé‡èŒƒæ•¸æ˜¯è¡¡é‡æ¬Šé‡å¤§å°çš„
        é‡è¦æŒ‡æ¨™ï¼Œå°æ–¼æ¨¡å‹ç©©å®šæ€§å’Œæ€§èƒ½è‡³é—œé‡è¦ã€‚
        
        åƒæ•¸:
            model (nn.Module): è¦æª¢æŸ¥çš„PyTorchæ¨¡å‹
        
        è¿”å›:
            Dict[str, Dict[str, float]]: åŒ…å«å„å±¤æ¬Šé‡çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
                æ¯å€‹å±¤çš„çµ±è¨ˆä¿¡æ¯åŒ…æ‹¬:
                - norm: æ¬Šé‡L2èŒƒæ•¸
                - mean: æ¬Šé‡å‡å€¼
                - std: æ¬Šé‡æ¨™æº–å·®
        
        æŠ€è¡“ç´°ç¯€:
        - åªæª¢æŸ¥åŒ…å«'weight'çš„åƒæ•¸
        - è¨ˆç®—L2èŒƒæ•¸ã€å‡å€¼å’Œæ¨™æº–å·®
        - æä¾›æ¬Šé‡åˆ†å¸ƒçµ±è¨ˆä¿¡æ¯
        - æ”¯æŒæ¬Šé‡ç•°å¸¸æª¢æ¸¬
        
        ä½¿ç”¨ç¤ºä¾‹:
            weight_stats = diagnostics.check_weight_norms(model)
            for layer_name, stats in weight_stats.items():
                if stats['norm'] > 100.0:
                    print(f"è­¦å‘Š: {layer_name} æ¬Šé‡èŒƒæ•¸éå¤§")
        """
        weight_stats = {}
        
        # éæ­·æ¨¡å‹ä¸­çš„æ‰€æœ‰åƒæ•¸
        for name, param in model.named_parameters():
            # åªè™•ç†æ¬Šé‡åƒæ•¸ (é€šå¸¸åŒ…å«'weight'é—œéµå­—)
            if 'weight' in name:
                # è¨ˆç®—æ¬Šé‡çš„L2èŒƒæ•¸
                weight_norm = param.data.norm(2).item()
                
                # è¨ˆç®—æ¬Šé‡çš„å‡å€¼
                weight_mean = param.data.mean().item()
                
                # è¨ˆç®—æ¬Šé‡çš„æ¨™æº–å·®
                weight_std = param.data.std().item()
                
                # è¨˜éŒ„è©²å±¤çš„æ¬Šé‡çµ±è¨ˆä¿¡æ¯
                weight_stats[name] = {
                    'norm': weight_norm,    # L2èŒƒæ•¸
                    'mean': weight_mean,    # å‡å€¼
                    'std': weight_std       # æ¨™æº–å·®
                }
        
        return weight_stats
    
    def check_activations(self, model: nn.Module, sample_input: torch.Tensor):
        """
        æª¢æŸ¥æ¿€æ´»å€¼åˆ†å¸ƒ
        
        é€™å€‹æ–¹æ³•ä½¿ç”¨PyTorchçš„hookæ©Ÿåˆ¶ä¾†æ•ç²æ¨¡å‹å‰å‘å‚³æ’­éç¨‹ä¸­å„å±¤çš„
        æ¿€æ´»å€¼ï¼Œä¸¦åˆ†æå…¶åˆ†å¸ƒçµ±è¨ˆä¿¡æ¯ã€‚æ¿€æ´»å€¼åˆ†æå°æ–¼æª¢æ¸¬æ¢¯åº¦æ¶ˆå¤±ã€
        æ¿€æ´»é£½å’Œã€æ•¸å€¼ä¸ç©©å®šç­‰å•é¡Œéå¸¸é‡è¦ã€‚
        
        åƒæ•¸:
            model (nn.Module): è¦æª¢æŸ¥çš„PyTorchæ¨¡å‹
            sample_input (torch.Tensor): ç”¨æ–¼å‰å‘å‚³æ’­çš„æ¨£æœ¬è¼¸å…¥
        
        è¿”å›:
            Dict[str, Dict[str, float]]: åŒ…å«å„å±¤æ¿€æ´»å€¼çµ±è¨ˆä¿¡æ¯çš„å­—å…¸
                æ¯å€‹å±¤çš„çµ±è¨ˆä¿¡æ¯åŒ…æ‹¬:
                - mean: æ¿€æ´»å€¼å‡å€¼
                - std: æ¿€æ´»å€¼æ¨™æº–å·®
                - max: æ¿€æ´»å€¼æœ€å¤§å€¼
                - min: æ¿€æ´»å€¼æœ€å°å€¼
                - nan_count: NaNå€¼æ•¸é‡
                - inf_count: Infå€¼æ•¸é‡
        
        æŠ€è¡“ç´°ç¯€:
        - ä½¿ç”¨forward hookæ•ç²æ¿€æ´»å€¼
        - åªç›£æ§è‘‰å­æ¨¡çµ„ (leaf modules)
        - è¨ˆç®—å®Œæ•´çš„çµ±è¨ˆä¿¡æ¯
        - æª¢æ¸¬æ•¸å€¼ç•°å¸¸ (NaN/Inf)
        - è‡ªå‹•æ¸…ç†hooké¿å…å…§å­˜æ´©æ¼
        
        ä½¿ç”¨ç¤ºä¾‹:
            sample_input = torch.randn(1, 784)
            activation_stats = diagnostics.check_activations(model, sample_input)
            for layer_name, stats in activation_stats.items():
                if stats['nan_count'] > 0:
                    print(f"è­¦å‘Š: {layer_name} åŒ…å«NaNå€¼")
        """
        activations = {}  # å­˜å„²å„å±¤æ¿€æ´»å€¼çµ±è¨ˆä¿¡æ¯
        handles = []      # å­˜å„²hookå¥æŸ„ï¼Œç”¨æ–¼å¾ŒçºŒæ¸…ç†
        
        def get_activation(name):
            """
            å‰µå»ºæ¿€æ´»å€¼æ•ç²hook
            
            é€™å€‹å…§éƒ¨å‡½æ•¸å‰µå»ºä¸€å€‹hookå‡½æ•¸ï¼Œç”¨æ–¼æ•ç²æŒ‡å®šæ¨¡çµ„çš„æ¿€æ´»å€¼
            ä¸¦è¨ˆç®—çµ±è¨ˆä¿¡æ¯ã€‚
            
            åƒæ•¸:
                name (str): æ¨¡çµ„åç¨±
            
            è¿”å›:
                function: hookå‡½æ•¸
            """
            def hook(model, input, output):
                # åªè™•ç†å¼µé‡è¼¸å‡º
                if isinstance(output, torch.Tensor):
                    activations[name] = {
                        'mean': output.mean().item(),      # æ¿€æ´»å€¼å‡å€¼
                        'std': output.std().item(),        # æ¿€æ´»å€¼æ¨™æº–å·®
                        'max': output.max().item(),        # æ¿€æ´»å€¼æœ€å¤§å€¼
                        'min': output.min().item(),        # æ¿€æ´»å€¼æœ€å°å€¼
                        'nan_count': torch.isnan(output).sum().item(),  # NaNå€¼æ•¸é‡
                        'inf_count': torch.isinf(output).sum().item()   # Infå€¼æ•¸é‡
                    }
            return hook
        
        # è¨»å†Šforward hookåˆ°æ‰€æœ‰è‘‰å­æ¨¡çµ„
        for name, module in model.named_modules():
            # åªç›£æ§è‘‰å­æ¨¡çµ„ (æ²’æœ‰å­æ¨¡çµ„çš„æ¨¡çµ„)
            if len(list(module.children())) == 0:
                handle = module.register_forward_hook(get_activation(name))
                handles.append(handle)
        
        # åŸ·è¡Œå‰å‘å‚³æ’­ä»¥è§¸ç™¼hook
        model.eval()  # è¨­ç½®ç‚ºè©•ä¼°æ¨¡å¼
        with torch.no_grad():  # ç¦ç”¨æ¢¯åº¦è¨ˆç®—
            _ = model(sample_input)
        
        # æ¸…ç†æ‰€æœ‰hookï¼Œé¿å…å…§å­˜æ´©æ¼
        for handle in handles:
            handle.remove()
            
        return activations
    
    def diagnose_instability(self, loss_history: List[float], threshold: float = 1.0) -> List[str]:
        """
        è¨ºæ–·è¨“ç·´ä¸ç©©å®šæ€§
        
        é€™å€‹æ–¹æ³•åˆ†ææå¤±å‡½æ•¸çš„æ­·å²è¨˜éŒ„ï¼Œæª¢æ¸¬å„ç¨®è¨“ç·´ä¸ç©©å®šæ€§å•é¡Œï¼Œ
        åŒ…æ‹¬æå¤±è·³èºã€æ•¸å€¼ç•°å¸¸ã€æŒ¯ç›ªæ¨¡å¼ç­‰ã€‚é€™äº›å•é¡Œé€šå¸¸è¡¨ç¤ºè¨“ç·´
        éç¨‹ä¸­å­˜åœ¨æ•¸å€¼ä¸ç©©å®šã€å­¸ç¿’ç‡éå¤§ã€æ¢¯åº¦å•é¡Œç­‰ã€‚
        
        åƒæ•¸:
            loss_history (List[float]): æå¤±å‡½æ•¸æ­·å²è¨˜éŒ„
            threshold (float): æå¤±è·³èºæª¢æ¸¬é–¾å€¼ï¼Œé»˜èªç‚º1.0
        
        è¿”å›:
            List[str]: æª¢æ¸¬åˆ°çš„å•é¡Œåˆ—è¡¨ï¼Œæ¯å€‹å•é¡ŒåŒ…å«æè¿°ä¿¡æ¯
        
        æª¢æ¸¬çš„å•é¡Œé¡å‹:
        1. æå¤±è·³èº (Loss Jump)
           - æª¢æ¸¬ç›¸é„°æ­¥é©Ÿé–“æå¤±å€¼çš„åŠ‡çƒˆè®ŠåŒ–
           - é€šå¸¸è¡¨ç¤ºå­¸ç¿’ç‡éå¤§æˆ–æ¢¯åº¦çˆ†ç‚¸
        
        2. æ•¸å€¼ç•°å¸¸ (Invalid Values)
           - æª¢æ¸¬NaN (Not a Number) å€¼
           - æª¢æ¸¬Inf (Infinity) å€¼
           - é€šå¸¸è¡¨ç¤ºæ•¸å€¼è¨ˆç®—æº¢å‡ºæˆ–é™¤é›¶éŒ¯èª¤
        
        3. æå¤±æŒ¯ç›ª (Loss Oscillation)
           - æª¢æ¸¬æå¤±å€¼çš„åŠ‡çƒˆæ³¢å‹•
           - é€šå¸¸è¡¨ç¤ºå­¸ç¿’ç‡ä¸ç©©å®šæˆ–æ•¸æ“šå•é¡Œ
        
        ä½¿ç”¨ç¤ºä¾‹:
            loss_history = [0.5, 0.3, 0.8, 0.2, 0.9]
            issues = diagnostics.diagnose_instability(loss_history, threshold=0.5)
            for issue in issues:
                print(f"å•é¡Œ: {issue}")
        """
        issues = []  # å­˜å„²æª¢æ¸¬åˆ°çš„å•é¡Œ
        
        # æª¢æŸ¥æ­·å²è¨˜éŒ„é•·åº¦
        if len(loss_history) < 2:
            return issues
        
        # 1. æª¢æŸ¥æå¤±è·³èº
        # æª¢æ¸¬ç›¸é„°æ­¥é©Ÿé–“æå¤±å€¼çš„åŠ‡çƒˆè®ŠåŒ–
        for i in range(1, len(loss_history)):
            loss_change = abs(loss_history[i] - loss_history[i-1])
            if loss_change > threshold:
                issues.append(
                    f"Loss jump detected at step {i}: "
                    f"{loss_history[i-1]:.4f} -> {loss_history[i]:.4f} "
                    f"(change: {loss_change:.4f})"
                )
        
        # 2. æª¢æŸ¥NaNæˆ–Infå€¼
        # æª¢æ¸¬æ•¸å€¼ç•°å¸¸ï¼Œé€™äº›é€šå¸¸è¡¨ç¤ºåš´é‡çš„æ•¸å€¼å•é¡Œ
        for i, loss in enumerate(loss_history):
            if np.isnan(loss):
                issues.append(f"NaN loss value detected at step {i}: {loss}")
            elif np.isinf(loss):
                issues.append(f"Infinity loss value detected at step {i}: {loss}")
        
        # 3. æª¢æŸ¥æå¤±æŒ¯ç›ª
        # æª¢æ¸¬æå¤±å€¼çš„åŠ‡çƒˆæ³¢å‹•æ¨¡å¼
        if len(loss_history) > 10:
            recent_losses = loss_history[-10:]  # å–æœ€è¿‘10å€‹æ­¥é©Ÿ
            loss_std = np.std(recent_losses)    # è¨ˆç®—æ¨™æº–å·®
            loss_mean = np.mean(recent_losses)  # è¨ˆç®—å‡å€¼
            
            # å¦‚æœæ¨™æº–å·®ç›¸å°æ–¼å‡å€¼éå¤§ï¼Œèªç‚ºå­˜åœ¨æŒ¯ç›ª
            if loss_std > loss_mean * 0.1:  # 10%çš„é–¾å€¼
                issues.append(
                    f"Loss oscillation detected in recent steps: "
                    f"std={loss_std:.4f}, mean={loss_mean:.4f}, "
                    f"cv={loss_std/loss_mean:.2%}"
                )
        
        return issues

class StableTrainer:
    """
    ç©©å®šè¨“ç·´å™¨é¡
    
    é€™å€‹é¡æä¾›äº†ä¸€å¥—å®Œæ•´çš„ç©©å®šè¨“ç·´è§£æ±ºæ–¹æ¡ˆï¼Œå°ˆé–€é‡å°æ·±åº¦å­¸ç¿’æ¨¡å‹
    è¨“ç·´éç¨‹ä¸­çš„å„ç¨®ç©©å®šæ€§å•é¡Œã€‚å®ƒé›†æˆäº†å¤šç¨®ç©©å®šåŒ–æŠ€è¡“ï¼ŒåŒ…æ‹¬æ¢¯åº¦
    è£å‰ªã€å­¸ç¿’ç‡èª¿åº¦ã€æ•¸å€¼ç©©å®šæ€§ä¿è­·ç­‰ã€‚
    
    ä¸»è¦åŠŸèƒ½:
    1. ç©©å®šåŒ–è¨“ç·´æŠ€è¡“
       - æ¢¯åº¦è£å‰ªå’Œæ­£å‰‡åŒ–
       - å­¸ç¿’ç‡é ç†±å’Œèª¿åº¦
       - æ•¸å€¼ç©©å®šæ€§ä¿è­·
       - æ··åˆç²¾åº¦è¨“ç·´æ”¯æŒ
    
    2. è¨“ç·´ç›£æ§å’Œè¨ºæ–·
       - å¯¦æ™‚æ¢¯åº¦ç›£æ§
       - æ¬Šé‡å¥åº·æª¢æŸ¥
       - æ¿€æ´»å€¼åˆ†æ
       - æå¤±ç©©å®šæ€§è¨ºæ–·
    
    3. å„ªåŒ–å™¨å’Œèª¿åº¦å™¨
       - æ”¯æŒå¤šç¨®å„ªåŒ–å™¨ (Adam, AdamW, SGD)
       - å­¸ç¿’ç‡èª¿åº¦ç­–ç•¥ (cosine, plateau, step)
       - è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´
       - æ¬Šé‡è¡°æ¸›æ­£å‰‡åŒ–
    
    4. éŒ¯èª¤è™•ç†å’Œæ¢å¾©
       - è‡ªå‹•ç•°å¸¸æª¢æ¸¬
       - è¨“ç·´ä¸­æ–·æ¢å¾©
       - æ•¸å€¼æº¢å‡ºä¿è­·
       - è©³ç´°æ—¥èªŒè¨˜éŒ„
    
    æŠ€è¡“ç‰¹é»:
    - ç”Ÿç”¢ç´šç©©å®šæ€§ä¿è­‰
    - å®Œæ•´çš„éŒ¯èª¤è™•ç†æ©Ÿåˆ¶
    - è©³ç´°çš„ç›£æ§å’Œè¨ºæ–·
    - æ”¯æŒå¤§è¦æ¨¡è¨“ç·´
    - å¯é…ç½®çš„ç©©å®šåŒ–åƒæ•¸
    
    ä½¿ç”¨ç¤ºä¾‹:
        model = MyModel()
        train_loader = DataLoader(train_dataset, batch_size=32)
        trainer = StableTrainer(model, train_loader)
        
        # è¨­ç½®å„ªåŒ–å™¨å’Œèª¿åº¦å™¨
        trainer.setup_optimizer_and_scheduler(
            learning_rate=1e-3,
            use_warmup=True,
            scheduler_type='cosine'
        )
        
        # é–‹å§‹è¨“ç·´
        for epoch in range(num_epochs):
            metrics = trainer.train_epoch(epoch)
            print(f"Epoch {epoch}: {metrics}")
    """
    
    def __init__(self, model: nn.Module, train_loader: DataLoader, 
                 val_loader: DataLoader = None, device: str = 'cpu'):
        """
        åˆå§‹åŒ–ç©©å®šè¨“ç·´å™¨
        
        åƒæ•¸:
            model (nn.Module): è¦è¨“ç·´çš„PyTorchæ¨¡å‹
            train_loader (DataLoader): è¨“ç·´æ•¸æ“šåŠ è¼‰å™¨
            val_loader (DataLoader, optional): é©—è­‰æ•¸æ“šåŠ è¼‰å™¨ï¼Œé»˜èªç‚ºNone
            device (str): è¨ˆç®—è¨­å‚™ï¼Œé»˜èªç‚º'cpu'
        
        åŠŸèƒ½:
        - åˆå§‹åŒ–æ¨¡å‹å’Œæ•¸æ“šåŠ è¼‰å™¨
        - è¨­ç½®è¨ºæ–·å·¥å…·
        - é…ç½®ç©©å®šåŒ–åƒæ•¸
        - æº–å‚™è¨“ç·´ç’°å¢ƒ
        """
        # å°‡æ¨¡å‹ç§»å‹•åˆ°æŒ‡å®šè¨­å‚™
        self.model = model.to(device)
        
        # è¨­ç½®æ•¸æ“šåŠ è¼‰å™¨
        self.train_loader = train_loader
        self.val_loader = val_loader
        
        # è¨­ç½®è¨ˆç®—è¨­å‚™
        self.device = device
        
        # åˆå§‹åŒ–è¨ºæ–·å·¥å…·
        self.diagnostics = TrainingStabilityDiagnostics()
        
        # ç©©å®šæ€§é…ç½®åƒæ•¸
        self.max_grad_norm = 1.0              # æ¢¯åº¦è£å‰ªé–¾å€¼
        self.loss_scale = 1.0                 # æå¤±ç¸®æ”¾å› å­ (æ··åˆç²¾åº¦è¨“ç·´)
        self.warmup_steps = 0                 # å­¸ç¿’ç‡é ç†±æ­¥æ•¸
        self.gradient_accumulation_steps = 1  # æ¢¯åº¦ç´¯ç©æ­¥æ•¸
        
    def setup_optimizer_and_scheduler(self, learning_rate: float = 1e-3,
                                    weight_decay: float = 1e-4,
                                    use_warmup: bool = True,
                                    scheduler_type: str = 'cosine'):
        """
        è¨­ç½®å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦å™¨
        
        é€™å€‹æ–¹æ³•é…ç½®è¨“ç·´éç¨‹ä¸­çš„å„ªåŒ–å™¨å’Œå­¸ç¿’ç‡èª¿åº¦ç­–ç•¥ï¼Œæä¾›å¤šç¨®
        ç©©å®šåŒ–æŠ€è¡“ä¾†æ”¹å–„è¨“ç·´æ•ˆæœå’Œç©©å®šæ€§ã€‚
        
        åƒæ•¸:
            learning_rate (float): åˆå§‹å­¸ç¿’ç‡ï¼Œé»˜èªç‚º1e-3
            weight_decay (float): æ¬Šé‡è¡°æ¸›ä¿‚æ•¸ï¼Œé»˜èªç‚º1e-4
            use_warmup (bool): æ˜¯å¦ä½¿ç”¨å­¸ç¿’ç‡é ç†±ï¼Œé»˜èªç‚ºTrue
            scheduler_type (str): å­¸ç¿’ç‡èª¿åº¦å™¨é¡å‹ï¼Œå¯é¸'cosine'æˆ–'plateau'
        
        æ”¯æŒçš„å„ªåŒ–å™¨:
        - AdamW: è‡ªé©æ‡‰å­¸ç¿’ç‡å„ªåŒ–å™¨ï¼Œå…·æœ‰æ¬Šé‡è¡°æ¸›æ­£å‰‡åŒ–
        - æ•¸å€¼ç©©å®šæ€§å„ªåŒ– (eps=1e-8)
        - å‹•é‡åƒæ•¸èª¿å„ª (betas=(0.9, 0.999))
        
        æ”¯æŒçš„èª¿åº¦å™¨:
        - CosineAnnealingLR: é¤˜å¼¦é€€ç«èª¿åº¦å™¨
        - ReduceLROnPlateau: å¹³å°é™ä½èª¿åº¦å™¨
        
        ä½¿ç”¨ç¤ºä¾‹:
            trainer.setup_optimizer_and_scheduler(
                learning_rate=1e-3,
                weight_decay=1e-4,
                use_warmup=True,
                scheduler_type='cosine'
            )
        """
        
        # ä½¿ç”¨AdamWå„ªåŒ–å™¨ï¼Œæä¾›æ›´å¥½çš„ç©©å®šæ€§
        # AdamWçµåˆäº†Adamçš„å„ªé»å’Œæ¬Šé‡è¡°æ¸›æ­£å‰‡åŒ–
        self.optimizer = optim.AdamW(
            self.model.parameters(),
            lr=learning_rate,           # å­¸ç¿’ç‡
            weight_decay=weight_decay,  # æ¬Šé‡è¡°æ¸›æ­£å‰‡åŒ–
            eps=1e-8,                   # æ•¸å€¼ç©©å®šæ€§åƒæ•¸
            betas=(0.9, 0.999)          # å‹•é‡åƒæ•¸
        )
        
        # è¨­ç½®å­¸ç¿’ç‡é ç†±
        if use_warmup:
            # é ç†±æ­¥æ•¸è¨­ç‚ºç¬¬ä¸€å€‹epochçš„10%
            self.warmup_steps = len(self.train_loader) // 10
        
        # è¨­ç½®å­¸ç¿’ç‡èª¿åº¦å™¨
        if scheduler_type == 'cosine':
            # é¤˜å¼¦é€€ç«èª¿åº¦å™¨ï¼šå­¸ç¿’ç‡æŒ‰é¤˜å¼¦å‡½æ•¸ä¸‹é™
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer, 
                T_max=len(self.train_loader) * 10  # å‡è¨­è¨“ç·´10å€‹epoch
            )
        elif scheduler_type == 'plateau':
            # å¹³å°é™ä½èª¿åº¦å™¨ï¼šç•¶æŒ‡æ¨™ä¸å†æ”¹å–„æ™‚é™ä½å­¸ç¿’ç‡
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer, 
                mode='min',      # ç›£æ§æŒ‡æ¨™è¶Šå°è¶Šå¥½
                factor=0.5,      # å­¸ç¿’ç‡ç¸®æ”¾å› å­
                patience=5,      # å®¹å¿æ­¥æ•¸
                verbose=True     # æ‰“å°èª¿åº¦ä¿¡æ¯
            )
        else:
            # ä¸ä½¿ç”¨èª¿åº¦å™¨
            self.scheduler = None
    
    def warmup_lr(self, step: int):
        """
        å­¸ç¿’ç‡é ç†±
        
        é€™å€‹æ–¹æ³•å¯¦ç¾å­¸ç¿’ç‡é ç†±æ©Ÿåˆ¶ï¼Œåœ¨è¨“ç·´åˆæœŸé€æ­¥å¢åŠ å­¸ç¿’ç‡ï¼Œ
        æœ‰åŠ©æ–¼ç©©å®šè¨“ç·´éç¨‹å’Œæ”¹å–„æ”¶æ–‚æ•ˆæœã€‚
        
        åƒæ•¸:
            step (int): ç•¶å‰è¨“ç·´æ­¥æ•¸
        
        é ç†±ç­–ç•¥:
        - ç·šæ€§é ç†±ï¼šå­¸ç¿’ç‡å¾0ç·šæ€§å¢åŠ åˆ°ç›®æ¨™å­¸ç¿’ç‡
        - é ç†±æ­¥æ•¸ï¼šåœ¨warmup_stepsæ­¥å…§å®Œæˆé ç†±
        - å¹³æ»‘éæ¸¡ï¼šé¿å…å­¸ç¿’ç‡çªç„¶è®ŠåŒ–
        
        æŠ€è¡“ç´°ç¯€:
        - åªåœ¨é ç†±éšæ®µèª¿æ•´å­¸ç¿’ç‡
        - é ç†±å› å­ = step / warmup_steps
        - é ç†±å®Œæˆå¾Œä½¿ç”¨æ­£å¸¸å­¸ç¿’ç‡
        
        ä½¿ç”¨ç¤ºä¾‹:
            for step in range(total_steps):
                trainer.warmup_lr(step)
                # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
        """
        # åªåœ¨é ç†±éšæ®µèª¿æ•´å­¸ç¿’ç‡
        if step < self.warmup_steps:
            # è¨ˆç®—é ç†±å› å­ï¼šå¾0ç·šæ€§å¢åŠ åˆ°1
            warmup_factor = step / self.warmup_steps
            
            # æ›´æ–°æ‰€æœ‰åƒæ•¸çµ„çš„å­¸ç¿’ç‡
            for param_group in self.optimizer.param_groups:
                # å­¸ç¿’ç‡ = åŸå§‹å­¸ç¿’ç‡ Ã— é ç†±å› å­
                param_group['lr'] = param_group['lr'] * warmup_factor
    
    def clip_gradients(self):
        """
        æ¢¯åº¦è£å‰ª
        
        é€™å€‹æ–¹æ³•å¯¦ç¾æ¢¯åº¦è£å‰ªæŠ€è¡“ï¼Œç”¨æ–¼é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å•é¡Œã€‚
        æ¢¯åº¦è£å‰ªé€šéé™åˆ¶æ¢¯åº¦èŒƒæ•¸ä¾†ä¿æŒè¨“ç·´ç©©å®šæ€§ã€‚
        
        æŠ€è¡“ç´°ç¯€:
        - ä½¿ç”¨L2èŒƒæ•¸é€²è¡Œæ¢¯åº¦è£å‰ª
        - åªè£å‰ªèŒƒæ•¸è¶…éé–¾å€¼çš„æ¢¯åº¦
        - ä¿æŒæ¢¯åº¦æ–¹å‘ä¸è®Š
        - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸å’Œæ•¸å€¼ä¸ç©©å®š
        
        è£å‰ªç­–ç•¥:
        - è¨ˆç®—æ‰€æœ‰åƒæ•¸æ¢¯åº¦çš„ç¸½èŒƒæ•¸
        - å¦‚æœç¸½èŒƒæ•¸è¶…émax_grad_normï¼Œå‰‡ç¸®æ”¾æ‰€æœ‰æ¢¯åº¦
        - ç¸®æ”¾å› å­ = max_grad_norm / å¯¦éš›èŒƒæ•¸
        
        ä½¿ç”¨ç¤ºä¾‹:
            # åœ¨åå‘å‚³æ’­å¾Œèª¿ç”¨
            loss.backward()
            trainer.clip_gradients()
            optimizer.step()
        """
        # åªåœ¨è¨­ç½®äº†æ¢¯åº¦è£å‰ªé–¾å€¼æ™‚åŸ·è¡Œ
        if self.max_grad_norm > 0:
            # ä½¿ç”¨PyTorchçš„æ¢¯åº¦è£å‰ªå‡½æ•¸
            torch.nn.utils.clip_grad_norm_(
                self.model.parameters(),  # è¦è£å‰ªçš„åƒæ•¸
                self.max_grad_norm        # æœ€å¤§æ¢¯åº¦èŒƒæ•¸é–¾å€¼
            )
    
    def check_model_health(self, step: int) -> bool:
        """
        æª¢æŸ¥æ¨¡å‹å¥åº·ç‹€æ…‹
        
        é€™å€‹æ–¹æ³•æª¢æŸ¥æ¨¡å‹åƒæ•¸æ˜¯å¦åŒ…å«ç•°å¸¸å€¼ï¼ˆNaNæˆ–Infï¼‰ï¼Œ
        ç”¨æ–¼æª¢æ¸¬æ•¸å€¼ä¸ç©©å®šå•é¡Œå’Œé˜²æ­¢è¨“ç·´å´©æ½°ã€‚
        
        åƒæ•¸:
            step (int): ç•¶å‰è¨“ç·´æ­¥æ•¸
        
        è¿”å›:
            bool: æ¨¡å‹æ˜¯å¦å¥åº·
                - True: æ¨¡å‹åƒæ•¸æ­£å¸¸
                - False: ç™¼ç¾ç•°å¸¸å€¼
        
        æª¢æŸ¥å…§å®¹:
        - NaN (Not a Number) å€¼æª¢æ¸¬
        - Inf (Infinity) å€¼æª¢æ¸¬
        - åƒæ•¸æ•¸å€¼ç¯„åœæª¢æŸ¥
        - ç•°å¸¸å€¼ä½ç½®è¨˜éŒ„
        
        æŠ€è¡“ç´°ç¯€:
        - ä½¿ç”¨torch.isnan()æª¢æ¸¬NaNå€¼
        - ä½¿ç”¨torch.isinf()æª¢æ¸¬Infå€¼
        - è¨˜éŒ„ç•°å¸¸åƒæ•¸åç¨±å’Œä½ç½®
        - æä¾›è©³ç´°çš„éŒ¯èª¤æ—¥èªŒ
        
        ä½¿ç”¨ç¤ºä¾‹:
            if not trainer.check_model_health(step):
                logger.error("æ¨¡å‹å¥åº·æª¢æŸ¥å¤±æ•—ï¼Œåœæ­¢è¨“ç·´")
                break
        """
        # æª¢æŸ¥æ‰€æœ‰æ¨¡å‹åƒæ•¸æ˜¯å¦åŒ…å«ç•°å¸¸å€¼
        for name, param in self.model.named_parameters():
            # æª¢æŸ¥NaNå€¼
            if torch.isnan(param).any():
                logger.error(f"NaN detected in parameter {name} at step {step}")
                return False
            
            # æª¢æŸ¥Infå€¼
            if torch.isinf(param).any():
                logger.error(f"Infinity detected in parameter {name} at step {step}")
                return False
        
        # æ‰€æœ‰åƒæ•¸éƒ½æ­£å¸¸
        return True
    
    def train_step(self, batch_idx: int, data: torch.Tensor, 
                   target: torch.Tensor) -> Dict[str, float]:
        """
        å–®æ­¥è¨“ç·´
        
        é€™å€‹æ–¹æ³•åŸ·è¡Œä¸€å€‹å®Œæ•´çš„è¨“ç·´æ­¥é©Ÿï¼ŒåŒ…æ‹¬å‰å‘å‚³æ’­ã€æå¤±è¨ˆç®—ã€
        åå‘å‚³æ’­ã€æ¢¯åº¦è£å‰ªå’Œåƒæ•¸æ›´æ–°ã€‚å®ƒé›†æˆäº†å¤šç¨®ç©©å®šåŒ–æŠ€è¡“ã€‚
        
        åƒæ•¸:
            batch_idx (int): ç•¶å‰æ‰¹æ¬¡ç´¢å¼•
            data (torch.Tensor): è¼¸å…¥æ•¸æ“š
            target (torch.Tensor): ç›®æ¨™æ¨™ç±¤
        
        è¿”å›:
            Dict[str, float]: è¨“ç·´æŒ‡æ¨™å­—å…¸
                - loss: åŸå§‹æå¤±å€¼
                - scaled_loss: ç¸®æ”¾å¾Œæå¤±å€¼
                - grad_norm: æ¢¯åº¦èŒƒæ•¸
                - lr: ç•¶å‰å­¸ç¿’ç‡
        
        è¨“ç·´æµç¨‹:
        1. å­¸ç¿’ç‡é ç†±
        2. å‰å‘å‚³æ’­
        3. æå¤±è¨ˆç®—
        4. æå¤±ç¸®æ”¾
        5. åå‘å‚³æ’­
        6. æ¢¯åº¦æª¢æŸ¥
        7. æ¢¯åº¦è£å‰ª
        8. åƒæ•¸æ›´æ–°
        9. å­¸ç¿’ç‡èª¿åº¦
        
        æŠ€è¡“ç‰¹é»:
        - æ”¯æŒå¤šç¨®æå¤±å‡½æ•¸
        - æ··åˆç²¾åº¦è¨“ç·´æ”¯æŒ
        - æ¢¯åº¦ç´¯ç©æ©Ÿåˆ¶
        - è‡ªå‹•æ¢¯åº¦ç›£æ§
        - å­¸ç¿’ç‡èª¿åº¦
        
        ä½¿ç”¨ç¤ºä¾‹:
            for batch_idx, (data, target) in enumerate(train_loader):
                metrics = trainer.train_step(batch_idx, data, target)
                print(f"Loss: {metrics['loss']:.4f}")
        """
        
        # 1. å­¸ç¿’ç‡é ç†±
        # åœ¨è¨“ç·´åˆæœŸé€æ­¥å¢åŠ å­¸ç¿’ç‡
        if hasattr(self, 'warmup_steps') and batch_idx < self.warmup_steps:
            self.warmup_lr(batch_idx)
        
        # 2. å‰å‘å‚³æ’­
        # è¨ˆç®—æ¨¡å‹è¼¸å‡º
        output = self.model(data)
        
        # 3. æå¤±è¨ˆç®—
        # æ ¹æ“šä»»å‹™é¡å‹é¸æ“‡åˆé©çš„æå¤±å‡½æ•¸
        if len(target.shape) > 1 and target.shape[1] > 1:  # å¤šæ¨™ç±¤åˆ†é¡
            loss = F.binary_cross_entropy_with_logits(output, target.float())
        else:  # å–®æ¨™ç±¤åˆ†é¡æˆ–å›æ­¸
            if target.dtype == torch.long:  # åˆ†é¡ä»»å‹™
                loss = F.cross_entropy(output, target)
            else:  # å›æ­¸ä»»å‹™
                loss = F.mse_loss(output, target)
        
        # 4. æå¤±ç¸®æ”¾ (æ··åˆç²¾åº¦è¨“ç·´)
        # ç”¨æ–¼é˜²æ­¢æ¢¯åº¦ä¸‹æº¢
        scaled_loss = loss * self.loss_scale
        
        # 5. åå‘å‚³æ’­
        # è¨ˆç®—æ¢¯åº¦
        scaled_loss.backward()
        
        # 6. æ¢¯åº¦ç´¯ç©å’Œåƒæ•¸æ›´æ–°
        # åªåœ¨ç´¯ç©æ­¥æ•¸é”åˆ°æ™‚æ›´æ–°åƒæ•¸
        if (batch_idx + 1) % self.gradient_accumulation_steps == 0:
            # æª¢æŸ¥æ¢¯åº¦èŒƒæ•¸
            grad_stats = self.diagnostics.check_gradient_norms(self.model)
            
            # æ¢¯åº¦è£å‰ª
            self.clip_gradients()
            
            # å„ªåŒ–å™¨æ­¥é©Ÿ
            self.optimizer.step()
            self.optimizer.zero_grad()
            
            # å­¸ç¿’ç‡èª¿åº¦
            if self.scheduler and not isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                self.scheduler.step()
        
        # è¿”å›è¨“ç·´æŒ‡æ¨™
        return {
            'loss': loss.item(),                    # åŸå§‹æå¤±å€¼
            'scaled_loss': scaled_loss.item(),      # ç¸®æ”¾å¾Œæå¤±å€¼
            'grad_norm': grad_stats.get('total_norm', 0.0),  # æ¢¯åº¦èŒƒæ•¸
            'lr': self.optimizer.param_groups[0]['lr']       # ç•¶å‰å­¸ç¿’ç‡
        }
    
    def validate(self) -> Dict[str, float]:
        """
        é©—è­‰æ­¥é©Ÿ
        
        é€™å€‹æ–¹æ³•åœ¨é©—è­‰é›†ä¸Šè©•ä¼°æ¨¡å‹æ€§èƒ½ï¼Œè¨ˆç®—æå¤±å’Œæº–ç¢ºç‡ç­‰æŒ‡æ¨™ã€‚
        é©—è­‰éç¨‹ä½¿ç”¨è©•ä¼°æ¨¡å¼ï¼Œç¦ç”¨æ¢¯åº¦è¨ˆç®—ä»¥æé«˜æ•ˆç‡ã€‚
        
        è¿”å›:
            Dict[str, float]: é©—è­‰æŒ‡æ¨™å­—å…¸
                - val_loss: å¹³å‡é©—è­‰æå¤±
                - val_accuracy: é©—è­‰æº–ç¢ºç‡
        
        é©—è­‰æµç¨‹:
        1. è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
        2. ç¦ç”¨æ¢¯åº¦è¨ˆç®—
        3. éæ­·é©—è­‰æ•¸æ“šé›†
        4. è¨ˆç®—æå¤±å’Œæº–ç¢ºç‡
        5. è¿”å›å¹³å‡æŒ‡æ¨™
        
        æŠ€è¡“ç‰¹é»:
        - è‡ªå‹•è¨­å‚™è½‰ç§»
        - æ”¯æŒå¤šç¨®ä»»å‹™é¡å‹
        - é«˜æ•ˆçš„é©—è­‰éç¨‹
        - è©³ç´°çš„æŒ‡æ¨™è¨ˆç®—
        
        ä½¿ç”¨ç¤ºä¾‹:
            val_metrics = trainer.validate()
            print(f"é©—è­‰æå¤±: {val_metrics['val_loss']:.4f}")
            print(f"é©—è­‰æº–ç¢ºç‡: {val_metrics['val_accuracy']:.4f}")
        """
        # æª¢æŸ¥æ˜¯å¦æœ‰é©—è­‰æ•¸æ“šåŠ è¼‰å™¨
        if not self.val_loader:
            return {}
        
        # è¨­ç½®æ¨¡å‹ç‚ºè©•ä¼°æ¨¡å¼
        self.model.eval()
        
        # åˆå§‹åŒ–çµ±è¨ˆè®Šé‡
        total_loss = 0    # ç¸½æå¤±
        correct = 0       # æ­£ç¢ºé æ¸¬æ•¸é‡
        total = 0         # ç¸½æ¨£æœ¬æ•¸é‡
        
        # ç¦ç”¨æ¢¯åº¦è¨ˆç®—ä»¥æé«˜æ•ˆç‡
        with torch.no_grad():
            # éæ­·é©—è­‰æ•¸æ“šé›†
            for data, target in self.val_loader:
                # å°‡æ•¸æ“šç§»å‹•åˆ°æŒ‡å®šè¨­å‚™
                data, target = data.to(self.device), target.to(self.device)
                
                # å‰å‘å‚³æ’­
                output = self.model(data)
                
                # è¨ˆç®—æå¤±
                if len(target.shape) > 1 and target.shape[1] > 1:  # å¤šæ¨™ç±¤åˆ†é¡
                    loss = F.binary_cross_entropy_with_logits(output, target.float())
                else:  # å–®æ¨™ç±¤åˆ†é¡æˆ–å›æ­¸
                    if target.dtype == torch.long:  # åˆ†é¡ä»»å‹™
                        loss = F.cross_entropy(output, target)
                        # è¨ˆç®—æº–ç¢ºç‡
                        pred = output.argmax(dim=1, keepdim=True)
                        correct += pred.eq(target.view_as(pred)).sum().item()
                    else:  # å›æ­¸ä»»å‹™
                        loss = F.mse_loss(output, target)
                
                # ç´¯åŠ çµ±è¨ˆä¿¡æ¯
                total_loss += loss.item()
                total += target.size(0)
        
        # è¨ˆç®—å¹³å‡æŒ‡æ¨™
        avg_loss = total_loss / len(self.val_loader)
        accuracy = correct / total if total > 0 else 0
        
        # è¿”å›é©—è­‰æŒ‡æ¨™
        return {
            'val_loss': avg_loss,      # å¹³å‡é©—è­‰æå¤±
            'val_accuracy': accuracy   # é©—è­‰æº–ç¢ºç‡
        }
    
    def train_epoch(self, epoch: int) -> Dict[str, float]:
        """
        è¨“ç·´ä¸€å€‹epoch
        
        é€™å€‹æ–¹æ³•åŸ·è¡Œä¸€å€‹å®Œæ•´çš„è¨“ç·´epochï¼ŒåŒ…æ‹¬æ•¸æ“šè™•ç†ã€è¨“ç·´æ­¥é©Ÿã€
        å¥åº·æª¢æŸ¥ã€ç©©å®šæ€§è¨ºæ–·å’Œé€²åº¦ç›£æ§ã€‚
        
        åƒæ•¸:
            epoch (int): ç•¶å‰è¨“ç·´epoch
        
        è¿”å›:
            Dict[str, float]: epochå¹³å‡æŒ‡æ¨™
                - avg_loss: å¹³å‡æå¤±
                - avg_grad_norm: å¹³å‡æ¢¯åº¦èŒƒæ•¸
                - avg_lr: å¹³å‡å­¸ç¿’ç‡
        
        è¨“ç·´æµç¨‹:
        1. è¨­ç½®æ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼
        2. åˆå§‹åŒ–æŒ‡æ¨™æ”¶é›†
        3. éæ­·è¨“ç·´æ•¸æ“š
        4. æ•¸æ“šè³ªé‡æª¢æŸ¥
        5. åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
        6. è¨˜éŒ„æŒ‡æ¨™
        7. å¥åº·ç‹€æ…‹æª¢æŸ¥
        8. ç©©å®šæ€§è¨ºæ–·
        9. é€²åº¦ç›£æ§
        10. è¨ˆç®—å¹³å‡æŒ‡æ¨™
        
        æŠ€è¡“ç‰¹é»:
        - è‡ªå‹•æ•¸æ“šè³ªé‡æª¢æŸ¥
        - å¯¦æ™‚å¥åº·ç›£æ§
        - ç©©å®šæ€§å•é¡Œè¨ºæ–·
        - è©³ç´°çš„é€²åº¦å ±å‘Š
        - è‡ªå‹•éŒ¯èª¤è™•ç†
        
        ä½¿ç”¨ç¤ºä¾‹:
            for epoch in range(num_epochs):
                metrics = trainer.train_epoch(epoch)
                print(f"Epoch {epoch}: {metrics}")
        """
        # è¨­ç½®æ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼
        self.model.train()
        
        # åˆå§‹åŒ–epochæŒ‡æ¨™æ”¶é›†
        epoch_metrics = {
            'loss': [],      # æå¤±å€¼åˆ—è¡¨
            'grad_norm': [], # æ¢¯åº¦èŒƒæ•¸åˆ—è¡¨
            'lr': []         # å­¸ç¿’ç‡åˆ—è¡¨
        }
        
        # éæ­·è¨“ç·´æ•¸æ“š
        for batch_idx, (data, target) in enumerate(self.train_loader):
            # å°‡æ•¸æ“šç§»å‹•åˆ°æŒ‡å®šè¨­å‚™
            data, target = data.to(self.device), target.to(self.device)
            
            # æª¢æŸ¥è¼¸å…¥æ•¸æ“šè³ªé‡
            if torch.isnan(data).any() or torch.isinf(data).any():
                logger.warning(f"NaN/Inf detected in input data at batch {batch_idx}")
                continue  # è·³éæœ‰å•é¡Œçš„æ‰¹æ¬¡
            
            # åŸ·è¡Œè¨“ç·´æ­¥é©Ÿ
            step_metrics = self.train_step(batch_idx, data, target)
            
            # è¨˜éŒ„æŒ‡æ¨™
            for key, value in step_metrics.items():
                if key in epoch_metrics:
                    epoch_metrics[key].append(value)
            
            # æª¢æŸ¥æ¨¡å‹å¥åº·ç‹€æ…‹
            if not self.check_model_health(batch_idx):
                logger.error(f"Training stopped due to model health issues at batch {batch_idx}")
                break  # åœæ­¢è¨“ç·´
            
            # è¨ºæ–·è¨“ç·´ä¸ç©©å®šæ€§
            if len(epoch_metrics['loss']) > 10:
                # ä½¿ç”¨æœ€è¿‘10å€‹æå¤±å€¼é€²è¡Œè¨ºæ–·
                issues = self.diagnostics.diagnose_instability(
                    epoch_metrics['loss'][-10:], 
                    threshold=np.mean(epoch_metrics['loss']) * 0.5
                )
                # è¨˜éŒ„ç™¼ç¾çš„å•é¡Œ
                for issue in issues:
                    logger.warning(f"Epoch {epoch}, Batch {batch_idx}: {issue}")
            
            # å®šæœŸæ‰“å°é€²åº¦
            if batch_idx % 100 == 0:
                logger.info(f"Epoch {epoch}, Batch {batch_idx}: "
                          f"Loss={step_metrics['loss']:.4f}, "
                          f"GradNorm={step_metrics['grad_norm']:.4f}, "
                          f"LR={step_metrics['lr']:.6f}")
        
        # è¨ˆç®—epochå¹³å‡æŒ‡æ¨™
        avg_metrics = {}
        for key, values in epoch_metrics.items():
            if values:  # ç¢ºä¿æœ‰æ•¸æ“š
                avg_metrics[f'avg_{key}'] = np.mean(values)
        
        return avg_metrics

def create_unstable_model():
    """
    å‰µå»ºä¸€å€‹å®¹æ˜“ä¸ç©©å®šçš„æ¨¡å‹ï¼ˆç”¨æ–¼æ¼”ç¤ºï¼‰
    
    é€™å€‹å‡½æ•¸å‰µå»ºä¸€å€‹æ•…æ„è¨­è¨ˆç‚ºä¸ç©©å®šçš„æ¨¡å‹ï¼Œç”¨æ–¼æ¼”ç¤ºè¨“ç·´
    ä¸ç©©å®šæ€§å•é¡Œå’Œè¨ºæ–·å·¥å…·çš„æ•ˆæœã€‚è©²æ¨¡å‹åŒ…å«å¤šç¨®å°è‡´ä¸ç©©å®š
    çš„è¨­è¨ˆç¼ºé™·ã€‚
    
    è¿”å›:
        nn.Module: ä¸ç©©å®šçš„æ¨¡å‹å¯¦ä¾‹
    
    è¨­è¨ˆç¼ºé™·:
    1. æ¬Šé‡åˆå§‹åŒ–å•é¡Œ
       - ä½¿ç”¨éå¤§çš„æ¨™æº–å·® (std=1.0)
       - å®¹æ˜“å°è‡´æ¢¯åº¦çˆ†ç‚¸
       - æ¬Šé‡åˆ†å¸ƒä¸åˆç†
    
    2. æ¿€æ´»å‡½æ•¸å•é¡Œ
       - ä½¿ç”¨tanhæ¿€æ´»å‡½æ•¸
       - å®¹æ˜“é£½å’Œå’Œæ¢¯åº¦æ¶ˆå¤±
       - ç¼ºä¹ç¾ä»£æ¿€æ´»å‡½æ•¸çš„å„ªé»
    
    3. ç¶²çµ¡æ¶æ§‹å•é¡Œ
       - ç¼ºä¹æ­£å‰‡åŒ–æŠ€è¡“
       - æ²’æœ‰æ‰¹æ­¸ä¸€åŒ–
       - æ²’æœ‰æ®˜å·®é€£æ¥
    
    4. æ•¸å€¼ç©©å®šæ€§å•é¡Œ
       - ç¼ºä¹æ¢¯åº¦è£å‰ª
       - æ²’æœ‰å­¸ç¿’ç‡èª¿åº¦
       - å®¹æ˜“æ•¸å€¼æº¢å‡º
    
    ä½¿ç”¨ç¤ºä¾‹:
        unstable_model = create_unstable_model()
        trainer = StableTrainer(unstable_model, train_loader)
        # è§€å¯Ÿè¨“ç·´ä¸ç©©å®šæ€§å•é¡Œ
    """
    class UnstableModel(nn.Module):
        """
        ä¸ç©©å®šæ¨¡å‹é¡
        
        é€™å€‹æ¨¡å‹æ•…æ„è¨­è¨ˆç‚ºä¸ç©©å®šï¼ŒåŒ…å«å¤šç¨®å°è‡´è¨“ç·´å•é¡Œçš„è¨­è¨ˆç¼ºé™·ã€‚
        ç”¨æ–¼æ¼”ç¤ºå’Œæ¸¬è©¦è¨“ç·´ç©©å®šæ€§è¨ºæ–·å·¥å…·ã€‚
        """
        def __init__(self):
            super().__init__()
            # å®šç¾©å…¨é€£æ¥å±¤
            # ä½¿ç”¨è¼ƒå¤§çš„åˆå§‹æ¬Šé‡ï¼ˆå®¹æ˜“æ¢¯åº¦çˆ†ç‚¸ï¼‰
            self.fc1 = nn.Linear(784, 512)
            self.fc2 = nn.Linear(512, 256)
            self.fc3 = nn.Linear(256, 128)
            self.fc4 = nn.Linear(128, 10)
            
            # ä¸ç•¶çš„æ¬Šé‡åˆå§‹åŒ–
            # ä½¿ç”¨éå¤§çš„æ¨™æº–å·®ï¼Œå®¹æ˜“å°è‡´æ¢¯åº¦çˆ†ç‚¸
            for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:
                nn.init.normal_(layer.weight, mean=0, std=1.0)  # éå¤§çš„æ¨™æº–å·®
        
        def forward(self, x):
            # å±•å¹³è¼¸å…¥
            x = x.view(x.size(0), -1)
            
            # ä½¿ç”¨tanhæ¿€æ´»å‡½æ•¸ï¼ˆå®¹æ˜“é£½å’Œï¼‰
            x = torch.tanh(self.fc1(x))  # tanhæ¿€æ´»å‡½æ•¸å®¹æ˜“é£½å’Œ
            x = torch.tanh(self.fc2(x))
            x = torch.tanh(self.fc3(x))
            
            # æœ€å¾Œä¸€å±¤ä¸ä½¿ç”¨æ¿€æ´»å‡½æ•¸
            x = self.fc4(x)
            return x
    
    return UnstableModel()

def create_stable_model():
    """
    å‰µå»ºä¸€å€‹ç©©å®šçš„æ¨¡å‹
    
    é€™å€‹å‡½æ•¸å‰µå»ºä¸€å€‹è¨­è¨ˆè‰¯å¥½çš„ç©©å®šæ¨¡å‹ï¼ŒåŒ…å«å¤šç¨®ç©©å®šåŒ–æŠ€è¡“
    å’Œæœ€ä½³å¯¦è¸ã€‚è©²æ¨¡å‹ç”¨æ–¼æ¼”ç¤ºç©©å®šè¨“ç·´çš„æ•ˆæœå’Œå°æ¯”ã€‚
    
    è¿”å›:
        nn.Module: ç©©å®šçš„æ¨¡å‹å¯¦ä¾‹
    
    ç©©å®šåŒ–æŠ€è¡“:
    1. æ‰¹æ­¸ä¸€åŒ– (Batch Normalization)
       - æ¨™æº–åŒ–æ¯å±¤çš„è¼¸å…¥
       - æ¸›å°‘å…§éƒ¨å”è®Šé‡åç§»
       - æé«˜è¨“ç·´ç©©å®šæ€§
    
    2. Dropoutæ­£å‰‡åŒ–
       - é˜²æ­¢éæ“¬åˆ
       - æé«˜æ³›åŒ–èƒ½åŠ›
       - å¢å¼·æ¨¡å‹é­¯æ£’æ€§
    
    3. æ­£ç¢ºçš„æ¬Šé‡åˆå§‹åŒ–
       - ä½¿ç”¨Xavieråˆå§‹åŒ–
       - ä¿æŒæ¢¯åº¦æ–¹å·®ç©©å®š
       - é¿å…æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±
    
    4. ç¾ä»£æ¿€æ´»å‡½æ•¸
       - ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•¸
       - é¿å…æ¢¯åº¦æ¶ˆå¤±å•é¡Œ
       - è¨ˆç®—æ•ˆç‡é«˜
    
    5. ç¶²çµ¡æ¶æ§‹å„ªåŒ–
       - åˆç†çš„å±¤æ•¸å’Œå¯¬åº¦
       - é©ç•¶çš„æ­£å‰‡åŒ–
       - è‰¯å¥½çš„æ¢¯åº¦æµå‹•
    
    ä½¿ç”¨ç¤ºä¾‹:
        stable_model = create_stable_model()
        trainer = StableTrainer(stable_model, train_loader)
        # è§€å¯Ÿç©©å®šè¨“ç·´æ•ˆæœ
    """
    class StableModel(nn.Module):
        """
        ç©©å®šæ¨¡å‹é¡
        
        é€™å€‹æ¨¡å‹æ¡ç”¨å¤šç¨®ç©©å®šåŒ–æŠ€è¡“å’Œæœ€ä½³å¯¦è¸ï¼Œæä¾›ç©©å®šå¯é çš„
        è¨“ç·´æ•ˆæœã€‚ç”¨æ–¼æ¼”ç¤ºç©©å®šè¨“ç·´çš„æ•ˆæœå’Œå°æ¯”ã€‚
        """
        def __init__(self):
            super().__init__()
            # å®šç¾©å…¨é€£æ¥å±¤
            self.fc1 = nn.Linear(784, 512)
            self.fc2 = nn.Linear(512, 256)
            self.fc3 = nn.Linear(256, 128)
            self.fc4 = nn.Linear(128, 10)
            
            # æ·»åŠ æ‰¹æ­¸ä¸€åŒ–å±¤
            self.bn1 = nn.BatchNorm1d(512)
            self.bn2 = nn.BatchNorm1d(256)
            self.bn3 = nn.BatchNorm1d(128)
            
            # æ·»åŠ Dropoutæ­£å‰‡åŒ–
            self.dropout = nn.Dropout(0.5)
            
            # æ­£ç¢ºçš„æ¬Šé‡åˆå§‹åŒ–
            for layer in [self.fc1, self.fc2, self.fc3, self.fc4]:
                nn.init.xavier_normal_(layer.weight)  # Xavieråˆå§‹åŒ–
                nn.init.zeros_(layer.bias)            # åç½®åˆå§‹åŒ–ç‚º0
        
        def forward(self, x):
            # å±•å¹³è¼¸å…¥
            x = x.view(x.size(0), -1)
            
            # ç¬¬ä¸€å±¤ï¼šå…¨é€£æ¥ + æ‰¹æ­¸ä¸€åŒ– + ReLU + Dropout
            x = F.relu(self.bn1(self.fc1(x)))
            x = self.dropout(x)
            
            # ç¬¬äºŒå±¤ï¼šå…¨é€£æ¥ + æ‰¹æ­¸ä¸€åŒ– + ReLU + Dropout
            x = F.relu(self.bn2(self.fc2(x)))
            x = self.dropout(x)
            
            # ç¬¬ä¸‰å±¤ï¼šå…¨é€£æ¥ + æ‰¹æ­¸ä¸€åŒ– + ReLU + Dropout
            x = F.relu(self.bn3(self.fc3(x)))
            x = self.dropout(x)
            
            # è¼¸å‡ºå±¤ï¼šå…¨é€£æ¥ï¼ˆä¸ä½¿ç”¨æ¿€æ´»å‡½æ•¸ï¼‰
            x = self.fc4(x)
            return x
    
    return StableModel()

def demonstrate_training_stability():
    """
    æ¼”ç¤ºè¨“ç·´ç©©å®šæ€§å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ
    
    é€™å€‹å‡½æ•¸æä¾›äº†ä¸€å€‹å®Œæ•´çš„è¨“ç·´ç©©å®šæ€§æ¼”ç¤ºï¼ŒåŒ…æ‹¬ï¼š
    1. ä¸ç©©å®šæ¨¡å‹è¨“ç·´æ¼”ç¤º
    2. ç©©å®šæ¨¡å‹è¨“ç·´æ¼”ç¤º
    3. è¨ºæ–·å·¥å…·æ¼”ç¤º
    4. å•é¡Œå°æ¯”å’Œåˆ†æ
    
    æ¼”ç¤ºå…§å®¹:
    - æ¢¯åº¦çˆ†ç‚¸å’Œæ¶ˆå¤±å•é¡Œ
    - æ•¸å€¼ä¸ç©©å®šæ€§å•é¡Œ
    - å­¸ç¿’ç‡è¨­ç½®å•é¡Œ
    - æ•¸æ“šè³ªé‡å•é¡Œ
    - ç©©å®šåŒ–æŠ€è¡“æ•ˆæœ
    
    æŠ€è¡“ç‰¹é»:
    - å°æ¯”å¼æ¼”ç¤º
    - è©³ç´°çš„è¨ºæ–·ä¿¡æ¯
    - å¯¦ç”¨çš„è§£æ±ºæ–¹æ¡ˆ
    - å®Œæ•´çš„éŒ¯èª¤è™•ç†
    
    ä½¿ç”¨ç¤ºä¾‹:
        demonstrate_training_stability()
    """
    print("=" * 80)
    print("ğŸ§  è¨“ç·´ç©©å®šæ€§å•é¡Œè¨ºæ–·èˆ‡è§£æ±ºæ¼”ç¤º")
    print("=" * 80)
    print("æœ¬æ¼”ç¤ºå°‡å±•ç¤ºè¨“ç·´ä¸ç©©å®šæ€§å•é¡Œå’Œç›¸æ‡‰çš„è§£æ±ºæ–¹æ¡ˆ")
    print("åŒ…æ‹¬æ¢¯åº¦å•é¡Œã€æ•¸å€¼ä¸ç©©å®šæ€§ã€å­¸ç¿’ç‡è¨­ç½®ç­‰")
    print("=" * 80)
    
    # =================================================================
    # 1. æº–å‚™æ¼”ç¤ºæ•¸æ“š
    # =================================================================
    print("\nğŸ“Š ç¬¬ä¸€æ­¥: æº–å‚™æ¼”ç¤ºæ•¸æ“š")
    print("-" * 50)
    print("æ­£åœ¨å‰µå»ºæ¨¡æ“¬æ•¸æ“šé›†...")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿å¯é‡ç¾æ€§
    torch.manual_seed(42)
    
    # å‰µå»ºæ¨¡æ“¬æ•¸æ“š
    X = torch.randn(1000, 784)  # 1000å€‹æ¨£æœ¬ï¼Œ784å€‹ç‰¹å¾µ
    y = torch.randint(0, 10, (1000,))  # 10å€‹é¡åˆ¥
    
    # æ·»åŠ ä¸€äº›"å£"æ•¸æ“šä¾†æ¨¡æ“¬æ•¸æ“šå•é¡Œ
    X[50:60] = float('nan')    # ä¸€äº›NaNæ•¸æ“š
    X[100:110] *= 1000         # ä¸€äº›ç•°å¸¸å¤§çš„æ•¸æ“š
    
    # å‰µå»ºæ•¸æ“šé›†å’Œæ•¸æ“šåŠ è¼‰å™¨
    dataset = TensorDataset(X, y)
    train_loader = DataLoader(dataset, batch_size=32, shuffle=True)
    
    print(f"   âœ… æ•¸æ“šæº–å‚™å®Œæˆ")
    print(f"   ğŸ“ æ¨£æœ¬æ•¸é‡: {len(dataset)}")
    print(f"   ğŸ“Š ç‰¹å¾µç¶­åº¦: {X.shape[1]}")
    print(f"   ğŸ¯ é¡åˆ¥æ•¸é‡: {len(torch.unique(y))}")
    print(f"   âš ï¸  åŒ…å«ç•°å¸¸æ•¸æ“š: NaNå’Œç•°å¸¸å€¼")
    
    # =================================================================
    # 2. ä¸ç©©å®šæ¨¡å‹è¨“ç·´æ¼”ç¤º
    # =================================================================
    print("\nğŸš¨ ç¬¬äºŒæ­¥: ä¸ç©©å®šæ¨¡å‹è¨“ç·´æ¼”ç¤º")
    print("-" * 50)
    print("æ­£åœ¨æ¼”ç¤ºä¸ç©©å®šæ¨¡å‹çš„è¨“ç·´å•é¡Œ...")
    
    # å‰µå»ºä¸ç©©å®šæ¨¡å‹
    unstable_model = create_unstable_model()
    unstable_trainer = StableTrainer(unstable_model, train_loader)
    
    # ä½¿ç”¨ä¸ç•¶çš„é…ç½®ï¼ˆæ•…æ„é€ æˆä¸ç©©å®šï¼‰
    unstable_trainer.setup_optimizer_and_scheduler(
        learning_rate=0.1,  # éå¤§çš„å­¸ç¿’ç‡
        use_warmup=False    # ä¸ä½¿ç”¨é ç†±
    )
    unstable_trainer.max_grad_norm = 0  # ä¸ä½¿ç”¨æ¢¯åº¦è£å‰ª
    
    print("   ğŸ”§ ä¸ç©©å®šé…ç½®:")
    print("      - å­¸ç¿’ç‡: 0.1 (éå¤§)")
    print("      - æ¢¯åº¦è£å‰ª: ç¦ç”¨")
    print("      - å­¸ç¿’ç‡é ç†±: ç¦ç”¨")
    print("      - æ¨¡å‹æ¶æ§‹: ä¸ç©©å®šè¨­è¨ˆ")
    
    try:
        # å˜—è©¦è¨“ç·´ä¸ç©©å®šæ¨¡å‹
        metrics = unstable_trainer.train_epoch(0)
        print("   âœ… ä¸ç©©å®šè¨“ç·´å®Œæˆ")
        for key, value in metrics.items():
            print(f"      {key}: {value:.4f}")
    except Exception as e:
        print(f"   âŒ ä¸ç©©å®šè¨“ç·´å¤±æ•—: {e}")
        print("      ğŸ’¡ é€™æ­£æ˜¯æˆ‘å€‘è¦æ¼”ç¤ºçš„å•é¡Œï¼")
    
    # =================================================================
    # 3. ç©©å®šæ¨¡å‹è¨“ç·´æ¼”ç¤º
    # =================================================================
    print("\nâœ… ç¬¬ä¸‰æ­¥: ç©©å®šæ¨¡å‹è¨“ç·´æ¼”ç¤º")
    print("-" * 50)
    print("æ­£åœ¨æ¼”ç¤ºç©©å®šæ¨¡å‹çš„è¨“ç·´æ•ˆæœ...")
    
    # å‰µå»ºç©©å®šæ¨¡å‹
    stable_model = create_stable_model()
    stable_trainer = StableTrainer(stable_model, train_loader)
    
    # ä½¿ç”¨åˆç†çš„é…ç½®
    stable_trainer.setup_optimizer_and_scheduler(
        learning_rate=1e-3,  # åˆç†çš„å­¸ç¿’ç‡
        use_warmup=True,     # ä½¿ç”¨é ç†±
        scheduler_type='cosine'  # ä½¿ç”¨é¤˜å¼¦èª¿åº¦
    )
    stable_trainer.max_grad_norm = 1.0  # ä½¿ç”¨æ¢¯åº¦è£å‰ª
    
    print("   ğŸ”§ ç©©å®šé…ç½®:")
    print("      - å­¸ç¿’ç‡: 1e-3 (åˆç†)")
    print("      - æ¢¯åº¦è£å‰ª: 1.0")
    print("      - å­¸ç¿’ç‡é ç†±: å•Ÿç”¨")
    print("      - æ¨¡å‹æ¶æ§‹: ç©©å®šè¨­è¨ˆ")
    
    # è¨“ç·´ç©©å®šæ¨¡å‹
    metrics = stable_trainer.train_epoch(0)
    print("   âœ… ç©©å®šè¨“ç·´å®Œæˆ")
    for key, value in metrics.items():
        print(f"      {key}: {value:.4f}")
    
    # =================================================================
    # 4. è¨ºæ–·å·¥å…·æ¼”ç¤º
    # =================================================================
    print("\nğŸ” ç¬¬å››æ­¥: è¨ºæ–·å·¥å…·æ¼”ç¤º")
    print("-" * 50)
    print("æ­£åœ¨æ¼”ç¤ºå„ç¨®è¨ºæ–·å·¥å…·...")
    
    # æº–å‚™æ¨£æœ¬æ•¸æ“š
    sample_input = torch.randn(1, 784)
    sample_target = torch.randint(0, 10, (1,))
    
    # è¨­ç½®æ¨¡å‹ç‚ºè¨“ç·´æ¨¡å¼
    stable_model.train()
    
    # åŸ·è¡Œå‰å‘å‚³æ’­å’Œåå‘å‚³æ’­
    output = stable_model(sample_input)
    loss = F.cross_entropy(output, sample_target)
    loss.backward()
    
    # 4.1 æ¢¯åº¦èŒƒæ•¸æª¢æŸ¥
    print("\n   ğŸ“Š æ¢¯åº¦èŒƒæ•¸æª¢æŸ¥:")
    grad_stats = stable_trainer.diagnostics.check_gradient_norms(stable_model)
    for key, value in grad_stats.items():
        if key != 'layer_norms':
            print(f"      {key}: {value:.6f}")
    
    # 4.2 æ¬Šé‡èŒƒæ•¸æª¢æŸ¥
    print("\n   âš–ï¸  æ¬Šé‡èŒƒæ•¸æª¢æŸ¥:")
    weight_stats = stable_trainer.diagnostics.check_weight_norms(stable_model)
    for name, stats in weight_stats.items():
        print(f"      {name}: norm={stats['norm']:.4f}, mean={stats['mean']:.4f}, std={stats['std']:.4f}")
    
    # 4.3 æ¿€æ´»å€¼æª¢æŸ¥
    print("\n   ğŸ§  æ¿€æ´»å€¼æª¢æŸ¥:")
    activation_stats = stable_trainer.diagnostics.check_activations(stable_model, sample_input)
    for name, stats in activation_stats.items():
        if stats['nan_count'] > 0 or stats['inf_count'] > 0:
            print(f"      {name}: âš ï¸  WARNING - NaN: {stats['nan_count']}, Inf: {stats['inf_count']}")
        else:
            print(f"      {name}: mean={stats['mean']:.4f}, std={stats['std']:.4f}")
    
    print("\n   âœ… è¨ºæ–·å·¥å…·æ¼”ç¤ºå®Œæˆ")
    print("   ğŸ’¡ é€™äº›å·¥å…·å¯ä»¥å¹«åŠ©è­˜åˆ¥å’Œè§£æ±ºè¨“ç·´ä¸ç©©å®šæ€§å•é¡Œ")

def common_fixes_summary():
    """
    å¸¸è¦‹ä¿®å¾©æ–¹æ³•ç¸½çµ
    
    é€™å€‹å‡½æ•¸æä¾›äº†ä¸€å€‹å…¨é¢çš„è¨“ç·´ä¸ç©©å®šæ€§å•é¡Œä¿®å¾©æ–¹æ³•ç¸½çµï¼Œ
    æ¶µè“‹äº†å¾æ•¸æ“šé è™•ç†åˆ°æ¨¡å‹æ¶æ§‹è¨­è¨ˆçš„å„å€‹æ–¹é¢ã€‚
    
    ä¿®å¾©æ–¹æ³•åˆ†é¡:
    1. å­¸ç¿’ç‡å•é¡Œä¿®å¾©
    2. æ¢¯åº¦å•é¡Œä¿®å¾©
    3. æ•¸æ“šå•é¡Œä¿®å¾©
    4. æ¨¡å‹æ¶æ§‹å„ªåŒ–
    5. åˆå§‹åŒ–ç­–ç•¥
    6. å„ªåŒ–å™¨é¸æ“‡
    7. æ•¸å€¼ç©©å®šæ€§ä¿è­·
    
    æŠ€è¡“ç‰¹é»:
    - ç³»çµ±åŒ–çš„å•é¡Œåˆ†é¡
    - å…·é«”çš„è§£æ±ºæ–¹æ¡ˆ
    - å¯¦ç”¨çš„å¯¦æ–½å»ºè­°
    - å®Œæ•´çš„è¦†è“‹ç¯„åœ
    
    ä½¿ç”¨ç¤ºä¾‹:
        common_fixes_summary()
    """
    print("\n" + "=" * 80)
    print("ğŸ”§ è¨“ç·´ä¸ç©©å®šæ€§å•é¡Œçš„å¸¸è¦‹ä¿®å¾©æ–¹æ³•")
    print("=" * 80)
    print("æœ¬ç¸½çµæ¶µè“‹äº†å¾æ•¸æ“šé è™•ç†åˆ°æ¨¡å‹æ¶æ§‹è¨­è¨ˆçš„å„å€‹æ–¹é¢")
    print("æä¾›äº†ç³»çµ±åŒ–çš„å•é¡Œè¨ºæ–·å’Œè§£æ±ºæ–¹æ¡ˆ")
    print("=" * 80)
    
    # å®šç¾©ä¿®å¾©æ–¹æ³•åˆ†é¡å’Œè§£æ±ºæ–¹æ¡ˆ
    fixes = {
        "å­¸ç¿’ç‡å•é¡Œ": [
            "é™ä½å­¸ç¿’ç‡ (1e-3 åˆ° 1e-5)",
            "ä½¿ç”¨å­¸ç¿’ç‡é ç†± (warmup)",
            "ä½¿ç”¨å­¸ç¿’ç‡èª¿åº¦å™¨ (cosine, plateau)",
            "ä¸åŒå±¤ä½¿ç”¨ä¸åŒå­¸ç¿’ç‡",
            "è‡ªé©æ‡‰å­¸ç¿’ç‡èª¿æ•´",
            "å­¸ç¿’ç‡è¡°æ¸›ç­–ç•¥"
        ],
        "æ¢¯åº¦å•é¡Œ": [
            "æ¢¯åº¦è£å‰ª (clip_grad_norm)",
            "æª¢æŸ¥æ¢¯åº¦èŒƒæ•¸",
            "ä½¿ç”¨gradient accumulation",
            "æª¢æŸ¥åå‘å‚³æ’­è·¯å¾‘",
            "æ¢¯åº¦æª¢æŸ¥å’Œèª¿è©¦",
            "æ¢¯åº¦æ­£å‰‡åŒ–æŠ€è¡“"
        ],
        "æ•¸æ“šå•é¡Œ": [
            "æ•¸æ“šæ­¸ä¸€åŒ–/æ¨™æº–åŒ–",
            "æª¢æŸ¥NaN/Infå€¼",
            "ç§»é™¤ç•°å¸¸å€¼",
            "æ•¸æ“šå¢å¼·è¦é©åº¦",
            "æ•¸æ“šè³ªé‡æª¢æŸ¥",
            "ç‰¹å¾µå·¥ç¨‹å„ªåŒ–"
        ],
        "æ¨¡å‹æ¶æ§‹": [
            "æ‰¹æ­¸ä¸€åŒ– (BatchNorm)",
            "å±¤æ­¸ä¸€åŒ– (LayerNorm)",
            "æ®˜å·®é€£æ¥ (ResNet)",
            "åˆé©çš„æ¿€æ´»å‡½æ•¸ (ReLU, GELU)",
            "æ³¨æ„åŠ›æ©Ÿåˆ¶",
            "æ­£å‰‡åŒ–æŠ€è¡“"
        ],
        "åˆå§‹åŒ–": [
            "Xavier/Glorotåˆå§‹åŒ–",
            "Heåˆå§‹åŒ–",
            "é¿å…å…¨é›¶åˆå§‹åŒ–",
            "æ¬Šé‡è¡°æ¸›æ­£å‰‡åŒ–",
            "åç½®åˆå§‹åŒ–ç­–ç•¥",
            "é è¨“ç·´æ¬Šé‡åŠ è¼‰"
        ],
        "å„ªåŒ–å™¨": [
            "ä½¿ç”¨Adam/AdamW",
            "èª¿æ•´momentumåƒæ•¸",
            "ä½¿ç”¨é©æ‡‰æ€§å­¸ç¿’ç‡",
            "è€ƒæ…®äºŒéšå„ªåŒ–å™¨",
            "å„ªåŒ–å™¨åƒæ•¸èª¿å„ª",
            "æ··åˆå„ªåŒ–ç­–ç•¥"
        ],
        "æ•¸å€¼ç©©å®šæ€§": [
            "æ··åˆç²¾åº¦è¨“ç·´",
            "æå¤±ç¸®æ”¾",
            "ä½¿ç”¨ç©©å®šçš„æå¤±å‡½æ•¸",
            "é¿å…é™¤é›¶æ“ä½œ",
            "æ•¸å€¼ç¯„åœæª¢æŸ¥",
            "æº¢å‡ºä¿è­·æ©Ÿåˆ¶"
        ]
    }
    
    # æ‰“å°ä¿®å¾©æ–¹æ³•ç¸½çµ
    for category, solutions in fixes.items():
        print(f"\nğŸ“‹ {category}:")
        print("-" * 40)
        for i, solution in enumerate(solutions, 1):
            print(f"   {i:2d}. {solution}")
    
    print("\n" + "=" * 80)
    print("ğŸ’¡ å¯¦æ–½å»ºè­°:")
    print("=" * 80)
    print("1. ç³»çµ±æ€§è¨ºæ–·: å¾æ•¸æ“šåˆ°æ¨¡å‹é€å±¤æª¢æŸ¥")
    print("2. æ¼¸é€²å¼ä¿®å¾©: ä¸€æ¬¡è§£æ±ºä¸€å€‹å•é¡Œ")
    print("3. å°æ¯”é©—è­‰: ä¿®å¾©å‰å¾Œæ•ˆæœå°æ¯”")
    print("4. æŒçºŒç›£æ§: å»ºç«‹é•·æœŸç©©å®šæ€§ç›£æ§")
    print("5. æ–‡æª”è¨˜éŒ„: è¨˜éŒ„å•é¡Œå’Œè§£æ±ºæ–¹æ¡ˆ")
    print("=" * 80)

if __name__ == "__main__":
    """
    ä¸»ç¨‹åºå…¥å£é»
    
    é€™å€‹è…³æœ¬å¯ä»¥ç›´æ¥é‹è¡Œä¾†æŸ¥çœ‹å®Œæ•´çš„è¨“ç·´ç©©å®šæ€§æ¼”ç¤ºã€‚
    å®ƒå°‡å±•ç¤ºï¼š
    - ä¸ç©©å®šæ¨¡å‹è¨“ç·´å•é¡Œ
    - ç©©å®šæ¨¡å‹è¨“ç·´æ•ˆæœ
    - è¨ºæ–·å·¥å…·ä½¿ç”¨æ–¹æ³•
    - å¸¸è¦‹ä¿®å¾©æ–¹æ³•ç¸½çµ
    - èª¿è©¦æµç¨‹å»ºè­°
    
    é‹è¡Œæ–¹å¼: python openAI_debug_instability.py
    
    é æœŸè¼¸å‡º:
    - å®Œæ•´çš„è¨“ç·´ç©©å®šæ€§æ¼”ç¤º
    - è©³ç´°çš„è¨ºæ–·ä¿¡æ¯
    - å¯¦ç”¨çš„ä¿®å¾©å»ºè­°
    - ç³»çµ±åŒ–çš„èª¿è©¦æµç¨‹
    
    æŠ€è¡“è¦æ±‚:
    - PyTorch >= 1.8.0
    - NumPy >= 1.19.0
    - Matplotlib >= 3.3.0 (å¯é¸ï¼Œç”¨æ–¼å¯è¦–åŒ–)
    """
    print("ğŸš€ å•Ÿå‹•è¨“ç·´ç©©å®šæ€§å•é¡Œèª¿è©¦å·¥å…·")
    print("=" * 80)
    
    # é‹è¡Œä¸»è¦æ¼”ç¤º
    demonstrate_training_stability()
    
    # é¡¯ç¤ºä¿®å¾©æ–¹æ³•ç¸½çµ
    common_fixes_summary()
    
    # èª¿è©¦æµç¨‹å»ºè­°
    print("\n" + "=" * 80)
    print("ğŸ” èª¿è©¦æµç¨‹å»ºè­°:")
    print("=" * 80)
    print("1. é¦–å…ˆæª¢æŸ¥æ•¸æ“šè³ªé‡ (NaN, Inf, ç•°å¸¸å€¼)")
    print("2. é©—è­‰æ¨¡å‹æ¶æ§‹å’Œåˆå§‹åŒ–")
    print("3. ç›£æ§æ¢¯åº¦å’Œæ¬Šé‡èŒƒæ•¸")
    print("4. èª¿æ•´å­¸ç¿’ç‡å’Œå„ªåŒ–å™¨è¨­ç½®")
    print("5. ä½¿ç”¨æ­£å‰‡åŒ–æŠ€è¡“ (dropout, weight decay)")
    print("6. å¯¦æ–½æ¢¯åº¦è£å‰ªå’Œå­¸ç¿’ç‡èª¿åº¦")
    print("7. æ·»åŠ æ•¸å€¼ç©©å®šæ€§ä¿è­·æªæ–½")
    print("8. ä½¿ç”¨è¨ºæ–·å·¥å…·æŒçºŒç›£æ§")
    print("=" * 80)
    
    print("\nğŸ‰ è¨“ç·´ç©©å®šæ€§å•é¡Œèª¿è©¦å·¥å…·æ¼”ç¤ºå®Œæˆ!")
    print("ğŸ’¡ å¸Œæœ›é€™äº›å·¥å…·èƒ½å¹«åŠ©æ‚¨è§£æ±ºè¨“ç·´ä¸ç©©å®šæ€§å•é¡Œ")
    print("=" * 80)