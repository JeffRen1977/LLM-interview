#!/usr/bin/env python3
"""
ML Pipeline Optimization Demo
Extracted from openAI_optimize_pipeline.md

This script demonstrates comprehensive ML pipeline optimization techniques including:
1. Performance profiling and analysis
2. Data loading and memory optimization
3. Feature engineering optimization
4. Model training optimization
5. Inference optimization
6. Performance comparison between optimized and basic pipelines
"""

import numpy as np
import pandas as pd
import time
from sklearn.datasets import make_classification
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.impute import SimpleImputer
import joblib
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from functools import partial
import cProfile
import io
import pstats
import warnings
import tempfile
import os
import sys

# Suppress warnings for cleaner output
warnings.filterwarnings("ignore")

class OptimizedMLPipeline:
    """ä¼˜åŒ–åŽçš„æœºå™¨å­¦ä¹ æµæ°´çº¿"""
    
    def __init__(self, use_gpu=False, n_jobs=-1):
        self.use_gpu = use_gpu
        self.n_jobs = n_jobs if n_jobs != -1 else mp.cpu_count()
        self.pipeline = None
        self.feature_cache = {}
        
    def profile_pipeline(self, func, *args, **kwargs):
        """æ€§èƒ½åˆ†æžå·¥å…·"""
        pr = cProfile.Profile()
        pr.enable()
        result = func(*args, **kwargs)
        pr.disable()
        
        s = io.StringIO()
        ps = pstats.Stats(pr, stream=s)
        ps.sort_stats('cumulative')
        ps.print_stats(10)  # æ‰“å°å‰10ä¸ªæœ€è€—æ—¶çš„å‡½æ•°
        print("\nðŸ“Š Profile Results:")
        print(s.getvalue())
        
        return result
    
    def optimize_data_loading(self, file_path=None, chunk_size=10000):
        """ä¼˜åŒ–æ•°æ®åŠ è½½ - ä½¿ç”¨åˆ†å—è¯»å–"""
        print("ðŸ“¦ Optimizing data loading...")
        
        if file_path:
            # åˆ†å—è¯»å–å¤§æ–‡ä»¶
            chunks = []
            for chunk in pd.read_csv(file_path, chunksize=chunk_size):
                # å¹¶è¡Œå¤„ç†æ¯ä¸ªchunk
                chunks.append(self._process_chunk(chunk))
            return pd.concat(chunks, ignore_index=True)
        else:
            # ç¤ºä¾‹æ•°æ®
            print("   Generating synthetic classification data...")
            X, y = make_classification(
                n_samples=100000, 
                n_features=20, 
                n_informative=15,
                n_redundant=5,
                n_classes=2,
                random_state=42
            )
            return pd.DataFrame(X), pd.Series(y)
    
    def _process_chunk(self, chunk):
        """å¤„ç†æ•°æ®å—"""
        # åŸºç¡€æ•°æ®æ¸…ç†
        chunk = chunk.dropna(thresh=len(chunk.columns) * 0.8)  # åˆ é™¤80%ä»¥ä¸Šç¼ºå¤±çš„è¡Œ
        return chunk
    
    def optimize_feature_engineering(self, X, y):
        """ä¼˜åŒ–ç‰¹å¾å·¥ç¨‹"""
        start_time = time.time()
        
        # 1. ç‰¹å¾ç¼“å­˜æœºåˆ¶
        feature_hash = hash(str(X.shape) + str(X.dtypes.to_list()))
        if feature_hash in self.feature_cache:
            print("   âœ… Using cached features")
            return self.feature_cache[feature_hash]
        
        # 2. å¹¶è¡Œç‰¹å¾å·¥ç¨‹
        print("ðŸ”§ Starting feature engineering optimization...")
        
        # æ•°å€¼ç‰¹å¾å’Œåˆ†ç±»ç‰¹å¾åˆ†ç¦»
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X.select_dtypes(exclude=[np.number]).columns.tolist()
        
        print(f"   ðŸ“Š Numeric features: {len(numeric_features)}")
        print(f"   ðŸ“Š Categorical features: {len(categorical_features)}")
        
        # æž„å»ºé¢„å¤„ç†pipeline
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),
            ('scaler', StandardScaler())
        ])
        
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),
        ])
        
        preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ],
            n_jobs=self.n_jobs
        )
        
        # åº”ç”¨é¢„å¤„ç†
        X_processed = preprocessor.fit_transform(X)
        
        # ç¼“å­˜ç»“æžœ
        self.feature_cache[feature_hash] = (X_processed, preprocessor)
        
        print(f"   âœ… Feature engineering completed in {time.time() - start_time:.2f}s")
        return X_processed, preprocessor
    
    def optimize_model_training(self, X, y):
        """ä¼˜åŒ–æ¨¡åž‹è®­ç»ƒ"""
        print("ðŸ¤– Starting model training optimization...")
        start_time = time.time()
        
        # 1. ä½¿ç”¨å¹¶è¡Œè®­ç»ƒ
        model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            n_jobs=self.n_jobs,  # å¹¶è¡Œè®­ç»ƒ
            random_state=42
        )
        
        # 2. æ—©åœç­–ç•¥ (è¿™é‡Œç”¨ç®€åŒ–ç‰ˆæœ¬)
        # å®žé™…ä¸­å¯ä»¥ä½¿ç”¨validation setè¿›è¡Œearly stopping
        
        model.fit(X, y)
        
        print(f"   âœ… Model training completed in {time.time() - start_time:.2f}s")
        return model
    
    def optimize_inference(self, model, X_test, batch_size=1000):
        """ä¼˜åŒ–æŽ¨ç†è¿‡ç¨‹"""
        print("âš¡ Starting batch inference optimization...")
        start_time = time.time()
        
        predictions = []
        
        # æ‰¹é‡æŽ¨ç†ä»¥å‡å°‘å†…å­˜ä½¿ç”¨
        for i in range(0, len(X_test), batch_size):
            batch = X_test[i:i+batch_size]
            batch_pred = model.predict(batch)
            predictions.extend(batch_pred)
        
        print(f"   âœ… Inference completed in {time.time() - start_time:.2f}s")
        return np.array(predictions)
    
    def memory_optimization(self, X):
        """å†…å­˜ä¼˜åŒ–"""
        print("ðŸ’¾ Starting memory optimization...")
        original_memory = X.memory_usage(deep=True).sum() / 1024**2
        
        # ä¼˜åŒ–æ•°æ®ç±»åž‹
        for col in X.select_dtypes(include=[np.number]).columns:
            if X[col].dtype == 'int64':
                if X[col].min() >= -128 and X[col].max() <= 127:
                    X[col] = X[col].astype('int8')
                elif X[col].min() >= -32768 and X[col].max() <= 32767:
                    X[col] = X[col].astype('int16')
                elif X[col].min() >= -2147483648 and X[col].max() <= 2147483647:
                    X[col] = X[col].astype('int32')
            elif X[col].dtype == 'float64':
                X[col] = pd.to_numeric(X[col], downcast='float')
        
        optimized_memory = X.memory_usage(deep=True).sum() / 1024**2
        memory_saved = ((original_memory - optimized_memory) / original_memory * 100)
        
        print(f"   âœ… Memory optimization: {original_memory:.2f}MB -> {optimized_memory:.2f}MB "
              f"(saved {memory_saved:.1f}%)")
        
        return X
    
    def gpu_acceleration_example(self, X, y):
        """GPUåŠ é€Ÿç¤ºä¾‹ (éœ€è¦å®‰è£…cuml)"""
        if not self.use_gpu:
            print("   â„¹ï¸  GPU acceleration not enabled")
            return None
            
        try:
            # è¿™é‡Œåªæ˜¯ç¤ºä¾‹ï¼Œå®žé™…ä½¿ç”¨éœ€è¦å®‰è£…cuml
            # from cuml.ensemble import RandomForestClassifier as cuRF
            # model = cuRF(n_estimators=100, max_depth=10)
            # model.fit(X, y)
            print("   â„¹ï¸  GPU acceleration example (requires cuml installation)")
        except ImportError:
            print("   âš ï¸  cuml not installed, GPU acceleration unavailable")
    
    def build_optimized_pipeline(self):
        """æž„å»ºå®Œæ•´çš„ä¼˜åŒ–æµæ°´çº¿"""
        print("=" * 60)
        print("ðŸš€ Building Optimized ML Pipeline")
        print("=" * 60)
        
        total_start_time = time.time()
        
        # 1. æ•°æ®åŠ è½½ä¼˜åŒ–
        print("\n1ï¸âƒ£  Optimizing data loading...")
        X, y = self.optimize_data_loading()
        print(f"   ðŸ“Š Data shape: {X.shape}")
        
        # 2. å†…å­˜ä¼˜åŒ–
        X = self.memory_optimization(X)
        
        # 3. æ•°æ®åˆ†å‰²
        print("\n2ï¸âƒ£  Splitting data...")
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        print(f"   ðŸ“Š Train set: {X_train.shape}, Test set: {X_test.shape}")
        
        # 4. ç‰¹å¾å·¥ç¨‹ä¼˜åŒ–
        print("\n3ï¸âƒ£  Optimizing feature engineering...")
        X_train_processed, preprocessor = self.optimize_feature_engineering(X_train, y_train)
        X_test_processed = preprocessor.transform(X_test)
        
        # 5. æ¨¡åž‹è®­ç»ƒä¼˜åŒ–
        print("\n4ï¸âƒ£  Optimizing model training...")
        model = self.optimize_model_training(X_train_processed, y_train)
        
        # 6. æŽ¨ç†ä¼˜åŒ–
        print("\n5ï¸âƒ£  Optimizing inference...")
        predictions = self.optimize_inference(model, X_test_processed)
        
        # 7. GPUåŠ é€Ÿç¤ºä¾‹
        print("\n6ï¸âƒ£  GPU acceleration example...")
        self.gpu_acceleration_example(X_train_processed, y_train)
        
        # 8. æ¨¡åž‹ä¿å­˜ä¼˜åŒ–
        print("\n7ï¸âƒ£  Saving optimized models...")
        with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as tmp_model:
            joblib.dump(model, tmp_model.name, compress=3)  # åŽ‹ç¼©ä¿å­˜
            model_size = os.path.getsize(tmp_model.name) / 1024**2
            os.unlink(tmp_model.name)
        
        with tempfile.NamedTemporaryFile(suffix='.joblib', delete=False) as tmp_prep:
            joblib.dump(preprocessor, tmp_prep.name, compress=3)
            prep_size = os.path.getsize(tmp_prep.name) / 1024**2
            os.unlink(tmp_prep.name)
        
        print(f"   ðŸ’¾ Model size: {model_size:.2f}MB, Preprocessor size: {prep_size:.2f}MB")
        
        total_time = time.time() - total_start_time
        accuracy = (predictions == y_test).mean()
        
        print("\n" + "=" * 60)
        print("ðŸ“ˆ PIPELINE OPTIMIZATION RESULTS")
        print("=" * 60)
        print(f"â±ï¸  Total time: {total_time:.2f}s")
        print(f"ðŸŽ¯ Prediction accuracy: {accuracy:.4f}")
        print(f"ðŸ’¾ Total model size: {model_size + prep_size:.2f}MB")
        
        return model, preprocessor, predictions

# æ€§èƒ½å¯¹æ¯”ç¤ºä¾‹
def compare_pipelines():
    """å¯¹æ¯”ä¼˜åŒ–å‰åŽçš„æ€§èƒ½"""
    print("\n" + "=" * 60)
    print("ðŸ“Š Performance Comparison: Basic vs Optimized Pipeline")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    print("\nðŸ“¦ Creating test data...")
    X, y = make_classification(n_samples=50000, n_features=20, random_state=42)
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    # æœªä¼˜åŒ–çš„åŸºç¡€æµæ°´çº¿
    print("\n1ï¸âƒ£  Basic Pipeline (Unoptimized)")
    start_time = time.time()
    
    basic_model = RandomForestClassifier(n_estimators=100, random_state=42)
    basic_model.fit(X_train, y_train)
    basic_pred = basic_model.predict(X_test)
    basic_time = time.time() - start_time
    basic_accuracy = (basic_pred == y_test).mean()
    
    print(f"   â±ï¸  Basic pipeline time: {basic_time:.2f}s")
    print(f"   ðŸŽ¯ Basic pipeline accuracy: {basic_accuracy:.4f}")
    
    # ä¼˜åŒ–åŽçš„æµæ°´çº¿
    print("\n2ï¸âƒ£  Optimized Pipeline")
    start_time = time.time()
    
    optimized_pipeline = OptimizedMLPipeline(n_jobs=mp.cpu_count())
    X_df, y_series = pd.DataFrame(X_train), pd.Series(y_train)
    X_processed, preprocessor = optimized_pipeline.optimize_feature_engineering(X_df, y_series)
    optimized_model = optimized_pipeline.optimize_model_training(X_processed, y_series)
    
    X_test_processed = preprocessor.transform(pd.DataFrame(X_test))
    optimized_pred = optimized_pipeline.optimize_inference(optimized_model, X_test_processed)
    optimized_time = time.time() - start_time
    optimized_accuracy = (optimized_pred == y_test).mean()
    
    print(f"   â±ï¸  Optimized pipeline time: {optimized_time:.2f}s")
    print(f"   ðŸŽ¯ Optimized pipeline accuracy: {optimized_accuracy:.4f}")
    
    # æ€§èƒ½æå‡
    speedup = basic_time / optimized_time
    time_saved = ((basic_time - optimized_time) / basic_time * 100)
    
    print("\n" + "=" * 60)
    print("ðŸ“ˆ PERFORMANCE IMPROVEMENT SUMMARY")
    print("=" * 60)
    print(f"ðŸš€ Speedup ratio: {speedup:.2f}x")
    print(f"â±ï¸  Time saved: {time_saved:.1f}%")
    print(f"ðŸŽ¯ Accuracy difference: {abs(optimized_accuracy - basic_accuracy):.4f}")
    
    if speedup > 1.5:
        print("   âœ… Significant performance improvement achieved!")
    if abs(optimized_accuracy - basic_accuracy) < 0.01:
        print("   âœ… Accuracy maintained while improving performance!")

def main():
    """Main function to run the pipeline optimization demo."""
    print("ðŸš€ ML Pipeline Optimization Demo")
    print("=" * 60)
    print("This demo showcases comprehensive ML pipeline optimization techniques")
    print("including data loading, feature engineering, model training, and inference optimization.")
    print("=" * 60)
    
    try:
        # è¿è¡Œä¼˜åŒ–æµæ°´çº¿
        pipeline = OptimizedMLPipeline(use_gpu=False, n_jobs=-1)
        
        # ä½¿ç”¨æ€§èƒ½åˆ†æž
        print("\nðŸ” Running pipeline with performance profiling...")
        pipeline.profile_pipeline(pipeline.build_optimized_pipeline)
        
        # è¿è¡Œæ€§èƒ½å¯¹æ¯”
        compare_pipelines()
        
        print("\n" + "=" * 60)
        print("ðŸ’¡ OPTIMIZATION RECOMMENDATIONS SUMMARY")
        print("=" * 60)
        print("1. ðŸ”„ Parallel Processing: Utilize multi-core CPU effectively")
        print("2. ðŸ’¾ Memory Optimization: Optimize data types to reduce memory usage")
        print("3. ðŸ“¦ Batch Processing: Avoid processing samples one by one")
        print("4. ðŸ—„ï¸  Feature Caching: Avoid redundant feature calculations")
        print("5. ðŸ—œï¸  Model Compression: Use compressed formats for model storage")
        print("6. ðŸš€ GPU Acceleration: Use GPU for large-scale data processing")
        print("7. âš¡ Pipeline Parallelism: Execute different stages in parallel")
        print("8. ðŸ“Š Performance Analysis: Regularly profile and optimize bottlenecks")
        
        print("\nðŸŽ‰ ML Pipeline optimization demo completed successfully!")
        return True
        
    except Exception as e:
        print(f"\nâŒ Error during pipeline optimization: {str(e)}")
        print("\nðŸ’¡ Troubleshooting tips:")
        print("   â€¢ Make sure all required packages are installed")
        print("   â€¢ Check if you have sufficient memory for the demo")
        print("   â€¢ Try reducing the dataset size if memory is limited")
        return False

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
