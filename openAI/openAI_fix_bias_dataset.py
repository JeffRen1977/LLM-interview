#!/usr/bin/env python3
"""
OpenAI Interview Question 6: Analyze and Fix a Biased Dataset

This comprehensive module demonstrates advanced techniques for detecting,
analyzing, and mitigating bias in machine learning datasets to ensure
fair and ethical AI systems.

Key Bias Analysis Techniques:
1. Bias Detection and Measurement
   - Statistical analysis of dataset composition
   - Demographic parity and equalized odds
   - Disparate impact analysis
   - Intersectional bias identification

2. Data-Level Bias Mitigation
   - Oversampling techniques (SMOTE, ADASYN)
   - Undersampling strategies (Random, Tomek Links)
   - Synthetic data generation
   - Data augmentation for fairness

3. Algorithm-Level Bias Mitigation
   - Fairness-aware loss functions
   - Constrained optimization
   - Adversarial debiasing
   - Post-processing techniques

4. Evaluation and Monitoring
   - Fairness metrics calculation
   - Bias monitoring dashboards
   - A/B testing for fairness
   - Continuous bias assessment

Technical Highlights:
- Comprehensive bias detection pipeline
- Multiple mitigation strategies
- Fairness metrics and evaluation
- Production-ready bias monitoring
- Real-world case studies and examples

Expected Outcomes:
- Clear understanding of bias types and sources
- Practical tools for bias detection and mitigation
- Fairness evaluation and monitoring techniques
- Production deployment considerations

Author: Jianfeng Ren
Date: 09/07/2025
Version: 2.0
"""

# Standard library imports
import warnings
warnings.filterwarnings('ignore')

# Third-party imports
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

# Scikit-learn imports
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score

# Imbalanced learning imports
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.combine import SMOTETomek

def main():
    """
    Main function to demonstrate comprehensive bias detection and mitigation techniques.
    
    This function showcases a complete pipeline for analyzing and fixing biased datasets,
    including bias detection, mitigation strategies, and fairness evaluation.
    
    Key Demonstration Areas:
    1. Bias Detection and Analysis
       - Statistical analysis of dataset composition
       - Visualization of bias patterns
       - Quantification of bias severity
       - Identification of bias sources
    
    2. Data-Level Bias Mitigation
       - Oversampling techniques (SMOTE)
       - Undersampling strategies
       - Combined sampling approaches
       - Synthetic data generation
    
    3. Algorithm-Level Bias Mitigation
       - Fairness-aware model training
       - Constrained optimization
       - Post-processing techniques
       - Bias monitoring and evaluation
    
    4. Fairness Evaluation
       - Multiple fairness metrics
       - Performance comparison
       - Bias reduction analysis
       - Production deployment considerations
    
    Expected Outcomes:
    - Clear understanding of bias types and sources
    - Practical tools for bias detection and mitigation
    - Fairness evaluation and monitoring techniques
    - Production deployment recommendations
    """
    
    print("ğŸ” æœ‰åè¦‹è³‡æ–™é›†åˆ†æèˆ‡ä¿®å¾©ç¶œåˆæ¼”ç¤º")
    print("=" * 80)
    print("æœ¬æ¼”ç¤ºå°‡å±•ç¤ºå¦‚ä½•æª¢æ¸¬ã€åˆ†æå’Œä¿®å¾©æ©Ÿå™¨å­¸ç¿’è³‡æ–™é›†ä¸­çš„åè¦‹")
    print("åŒ…æ‹¬è³‡æ–™å±¤é¢å’Œæ¼”ç®—æ³•å±¤é¢çš„åè¦‹ç·©è§£æŠ€è¡“")
    print("=" * 80)
    
    # =================================================================
    # 1. å‰µå»ºæ¨¡æ“¬çš„åè¦‹è³‡æ–™é›†
    # =================================================================
    print("\nğŸ“Š ç¬¬ä¸€æ­¥: å‰µå»ºæ¨¡æ“¬çš„åè¦‹è³‡æ–™é›†")
    print("-" * 50)
    print("æ­£åœ¨å‰µå»ºä¸€å€‹æ¨¡æ“¬çš„ã€æœ‰åš´é‡é¡åˆ¥ä¸å¹³è¡¡çš„è³‡æ–™é›†...")
    print("   ğŸ“ å ´æ™¯: ä¿¡ç”¨é•ç´„é æ¸¬")
    print("   ğŸ¯ ç›®æ¨™: é æ¸¬å®¢æˆ¶æ˜¯å¦æœƒé•ç´„")
    print("   âš ï¸  å•é¡Œ: åš´é‡é¡åˆ¥ä¸å¹³è¡¡ (95% éé•ç´„, 5% é•ç´„)")
    
    # è¨­ç½®éš¨æ©Ÿç¨®å­ä»¥ç¢ºä¿çµæœå¯é‡ç¾
    np.random.seed(42)
    
    # å‰µå»ºæ¨¡æ“¬è³‡æ–™
    # ç‰¹å¾µ1: æ”¶å…¥æ°´å¹³ (0-10)
    # ç‰¹å¾µ2: ä¿¡ç”¨è©•åˆ† (0-5)
    # ç›®æ¨™: é•ç´„æ¨™ç±¤ (0=éé•ç´„, 1=é•ç´„)
    data = {
        'feature1': np.random.rand(1000) * 10,  # æ”¶å…¥æ°´å¹³
        'feature2': np.random.rand(1000) * 5,   # ä¿¡ç”¨è©•åˆ†
        'target': [0] * 950 + [1] * 50  # åš´é‡ä¸å¹³è¡¡: 95% vs 5%
    }
    
    # è½‰æ›ç‚ºDataFrameä¸¦æ‰“äº‚é †åº
    df = pd.DataFrame(data)
    np.random.shuffle(df.values)
    
    # åˆ†é›¢ç‰¹å¾µå’Œç›®æ¨™
    X = df[['feature1', 'feature2']]
    y = df['target']
    
    print(f"   âœ… è³‡æ–™é›†å‰µå»ºå®Œæˆ")
    print(f"   ğŸ“Š è³‡æ–™å½¢ç‹€: {df.shape}")
    print(f"   ğŸ¯ ç‰¹å¾µæ•¸é‡: {X.shape[1]}")
    print(f"   ğŸ“ˆ æ¨£æœ¬æ•¸é‡: {len(y)}")
    
    # =================================================================
    # 2. æª¢æ¸¬åè¦‹ï¼šåˆ†æé¡åˆ¥åˆ†ä½ˆ
    # =================================================================
    print("\nğŸ” ç¬¬äºŒæ­¥: æª¢æ¸¬åè¦‹")
    print("-" * 50)
    print("æ­£åœ¨åˆ†æè³‡æ–™é›†çš„é¡åˆ¥åˆ†ä½ˆå’Œåè¦‹ç¨‹åº¦...")
    
    # åˆ†æé¡åˆ¥åˆ†ä½ˆ
    class_counts = y.value_counts()
    imbalance_ratio = class_counts[0] / class_counts[1]
    
    print(f"   ğŸ“Š åŸå§‹è³‡æ–™åˆ†ä½ˆ:")
    print(f"      - é¡åˆ¥ 0 (éé•ç´„): {class_counts[0]} æ¨£æœ¬ ({class_counts[0]/len(y)*100:.1f}%)")
    print(f"      - é¡åˆ¥ 1 (é•ç´„): {class_counts[1]} æ¨£æœ¬ ({class_counts[1]/len(y)*100:.1f}%)")
    print(f"      - ä¸å¹³è¡¡æ¯”ä¾‹: {imbalance_ratio:.1f}:1")
    
    # è©•ä¼°åè¦‹åš´é‡ç¨‹åº¦
    if imbalance_ratio > 10:
        bias_severity = "åš´é‡"
        print(f"   âš ï¸  åè¦‹åš´é‡ç¨‹åº¦: {bias_severity} (æ¯”ä¾‹ > 10:1)")
    elif imbalance_ratio > 5:
        bias_severity = "ä¸­ç­‰"
        print(f"   âš ï¸  åè¦‹åš´é‡ç¨‹åº¦: {bias_severity} (æ¯”ä¾‹ > 5:1)")
    else:
        bias_severity = "è¼•å¾®"
        print(f"   âœ… åè¦‹åš´é‡ç¨‹åº¦: {bias_severity} (æ¯”ä¾‹ â‰¤ 5:1)")
    
    # å‰µå»ºè¦–è¦ºåŒ–
    print("   ğŸ“ˆ å‰µå»ºåè¦‹è¦–è¦ºåŒ–åœ–è¡¨...")
    plt.figure(figsize=(15, 6))
    
    # åŸå§‹è³‡æ–™åˆ†ä½ˆ
    plt.subplot(1, 3, 1)
    sns.countplot(x='target', data=df)
    plt.title("åŸå§‹é¡åˆ¥åˆ†ä½ˆ\n(åš´é‡ä¸å¹³è¡¡)")
    plt.xlabel("é¡åˆ¥")
    plt.ylabel("æ¨£æœ¬æ•¸é‡")
    plt.xticks([0, 1], ['éé•ç´„', 'é•ç´„'])
    
    # æ·»åŠ æ•¸å€¼æ¨™ç±¤
    for i, v in enumerate(class_counts.values):
        plt.text(i, v + 10, str(v), ha='center', va='bottom', fontweight='bold')
    
    # ç‰¹å¾µåˆ†ä½ˆåˆ†æ
    plt.subplot(1, 3, 2)
    sns.boxplot(data=df, x='target', y='feature1')
    plt.title("ç‰¹å¾µ1åˆ†ä½ˆ\n(æ”¶å…¥æ°´å¹³)")
    plt.xlabel("é¡åˆ¥")
    plt.ylabel("ç‰¹å¾µ1å€¼")
    plt.xticks([0, 1], ['éé•ç´„', 'é•ç´„'])
    
    plt.subplot(1, 3, 3)
    sns.boxplot(data=df, x='target', y='feature2')
    plt.title("ç‰¹å¾µ2åˆ†ä½ˆ\n(ä¿¡ç”¨è©•åˆ†)")
    plt.xlabel("é¡åˆ¥")
    plt.ylabel("ç‰¹å¾µ2å€¼")
    plt.xticks([0, 1], ['éé•ç´„', 'é•ç´„'])
    
    plt.tight_layout()
    plt.savefig('bias_dataset_analysis.png', dpi=300, bbox_inches='tight')
    print("   âœ… è¦–è¦ºåŒ–åœ–è¡¨å·²ä¿å­˜ç‚º 'bias_dataset_analysis.png'")
    plt.show()
    
    # =================================================================
    # 3. åœ¨åè¦‹è³‡æ–™ä¸Šè¨“ç·´æ¨¡å‹ä¸¦è©•ä¼°
    # =================================================================
    print("\nğŸ¤– ç¬¬ä¸‰æ­¥: åœ¨åè¦‹è³‡æ–™ä¸Šè¨“ç·´æ¨¡å‹")
    print("-" * 50)
    print("æ­£åœ¨ä½¿ç”¨åè¦‹è³‡æ–™è¨“ç·´æ¨¡å‹ä¸¦è©•ä¼°å…¶è¡¨ç¾...")
    
    # åˆ†å‰²è³‡æ–™é›†
    print("   ğŸ“Š åˆ†å‰²è³‡æ–™é›†...")
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y
    )
    
    print(f"      - è¨“ç·´é›†å¤§å°: {X_train.shape[0]} æ¨£æœ¬")
    print(f"      - æ¸¬è©¦é›†å¤§å°: {X_test.shape[0]} æ¨£æœ¬")
    print(f"      - ç‰¹å¾µæ•¸é‡: {X_train.shape[1]}")
    
    # è¨“ç·´æ¨¡å‹
    print("   ğŸ”„ è¨“ç·´é‚è¼¯å›æ­¸æ¨¡å‹...")
    model_biased = LogisticRegression(random_state=42, max_iter=1000)
    model_biased.fit(X_train, y_train)
    
    # é æ¸¬å’Œè©•ä¼°
    print("   ğŸ“ˆ è©•ä¼°æ¨¡å‹è¡¨ç¾...")
    y_pred_biased = model_biased.predict(X_test)
    
    # è¨ˆç®—è©³ç´°æŒ‡æ¨™
    accuracy_biased = accuracy_score(y_test, y_pred_biased)
    f1_biased = f1_score(y_test, y_pred_biased)
    
    print(f"\n   ğŸ“Š åè¦‹æ¨¡å‹è¡¨ç¾:")
    print(f"      - æº–ç¢ºç‡: {accuracy_biased:.4f}")
    print(f"      - F1åˆ†æ•¸: {f1_biased:.4f}")
    
    # è©³ç´°åˆ†é¡å ±å‘Š
    print(f"\n   ğŸ“‹ è©³ç´°åˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred_biased, target_names=['éé•ç´„', 'é•ç´„']))
    
    # åˆ†æåè¦‹å½±éŸ¿
    print(f"\n   âš ï¸  åè¦‹å½±éŸ¿åˆ†æ:")
    print(f"      - å„˜ç®¡ç¸½é«”æº–ç¢ºç‡å¾ˆé«˜ ({accuracy_biased:.1%})ï¼Œä½†å°å°‘æ•¸é¡åˆ¥çš„è­˜åˆ¥èƒ½åŠ›å¾ˆå·®")
    print(f"      - F1åˆ†æ•¸è¼ƒä½ ({f1_biased:.4f}) è¡¨æ˜æ¨¡å‹åœ¨å¹³è¡¡ç²¾ç¢ºç‡å’Œå¬å›ç‡æ–¹é¢è¡¨ç¾ä¸ä½³")
    print(f"      - é€™æ˜¯ç”±æ–¼é¡åˆ¥ä¸å¹³è¡¡å°è‡´çš„åè¦‹å•é¡Œ")
    
    # =================================================================
    # 4. åè¦‹ç·©è§£ï¼šä½¿ç”¨SMOTEé‡æ¡æ¨£
    # =================================================================
    print("\nğŸ”§ ç¬¬å››æ­¥: åè¦‹ç·©è§£")
    print("-" * 50)
    print("æ­£åœ¨ä½¿ç”¨SMOTE (Synthetic Minority Oversampling Technique) é‡æ¡æ¨£...")
    print("   ğŸ“ æŠ€è¡“: SMOTE - åˆæˆå°‘æ•¸é¡éæ¡æ¨£æŠ€è¡“")
    print("   ğŸ¯ ç›®æ¨™: å¹³è¡¡é¡åˆ¥åˆ†ä½ˆï¼Œæ¸›å°‘åè¦‹")
    print("   âš™ï¸  åŸç†: åœ¨å°‘æ•¸é¡æ¨£æœ¬ä¹‹é–“ç”Ÿæˆåˆæˆæ¨£æœ¬")
    
    # æ‡‰ç”¨SMOTEé‡æ¡æ¨£
    print("   ğŸ”„ æ‡‰ç”¨SMOTEé‡æ¡æ¨£...")
    smote = SMOTE(random_state=42)
    X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)
    
    # åˆ†æé‡æ¡æ¨£çµæœ
    resampled_counts = pd.Series(y_train_resampled).value_counts()
    resampled_ratio = resampled_counts[0] / resampled_counts[1]
    
    print(f"\n   ğŸ“Š é‡æ¡æ¨£çµæœ:")
    print(f"      - åŸå§‹è¨“ç·´é›†å¤§å°: {X_train.shape[0]} æ¨£æœ¬")
    print(f"      - é‡æ¡æ¨£å¾Œå¤§å°: {X_train_resampled.shape[0]} æ¨£æœ¬")
    print(f"      - é¡åˆ¥ 0 (éé•ç´„): {resampled_counts[0]} æ¨£æœ¬")
    print(f"      - é¡åˆ¥ 1 (é•ç´„): {resampled_counts[1]} æ¨£æœ¬")
    print(f"      - æ–°é¡åˆ¥æ¯”ä¾‹: {resampled_ratio:.1f}:1")
    
    # è©•ä¼°é‡æ¡æ¨£æ•ˆæœ
    if resampled_ratio < 2.0:
        print(f"   âœ… é‡æ¡æ¨£æˆåŠŸ! é¡åˆ¥æ¯”ä¾‹å·²å¹³è¡¡åˆ° {resampled_ratio:.1f}:1")
    else:
        print(f"   âš ï¸  é‡æ¡æ¨£éƒ¨åˆ†æˆåŠŸï¼Œé¡åˆ¥æ¯”ä¾‹ç‚º {resampled_ratio:.1f}:1")
    
    # å‰µå»ºé‡æ¡æ¨£å‰å¾Œå°æ¯”è¦–è¦ºåŒ–
    print("   ğŸ“ˆ å‰µå»ºé‡æ¡æ¨£å‰å¾Œå°æ¯”åœ–...")
    plt.figure(figsize=(15, 5))
    
    # åŸå§‹è¨“ç·´è³‡æ–™åˆ†ä½ˆ
    plt.subplot(1, 3, 1)
    sns.countplot(x=y_train)
    plt.title("åŸå§‹è¨“ç·´è³‡æ–™åˆ†ä½ˆ\n(åè¦‹åš´é‡)")
    plt.xlabel("é¡åˆ¥")
    plt.ylabel("æ¨£æœ¬æ•¸é‡")
    plt.xticks([0, 1], ['éé•ç´„', 'é•ç´„'])
    
    # é‡æ¡æ¨£å¾Œè³‡æ–™åˆ†ä½ˆ
    plt.subplot(1, 3, 2)
    sns.countplot(x=y_train_resampled)
    plt.title("SMOTEé‡æ¡æ¨£å¾Œåˆ†ä½ˆ\n(é¡åˆ¥å¹³è¡¡)")
    plt.xlabel("é¡åˆ¥")
    plt.ylabel("æ¨£æœ¬æ•¸é‡")
    plt.xticks([0, 1], ['éé•ç´„', 'é•ç´„'])
    
    # ç‰¹å¾µåˆ†ä½ˆå°æ¯”
    plt.subplot(1, 3, 3)
    plt.scatter(X_train_resampled[y_train_resampled==0, 0], 
                X_train_resampled[y_train_resampled==0, 1], 
                alpha=0.6, label='éé•ç´„', s=20)
    plt.scatter(X_train_resampled[y_train_resampled==1, 0], 
                X_train_resampled[y_train_resampled==1, 1], 
                alpha=0.6, label='é•ç´„', s=20)
    plt.title("é‡æ¡æ¨£å¾Œç‰¹å¾µåˆ†ä½ˆ")
    plt.xlabel("ç‰¹å¾µ1 (æ”¶å…¥æ°´å¹³)")
    plt.ylabel("ç‰¹å¾µ2 (ä¿¡ç”¨è©•åˆ†)")
    plt.legend()
    
    plt.tight_layout()
    plt.savefig('bias_dataset_analysis.png', dpi=300, bbox_inches='tight')
    print("   âœ… é‡æ¡æ¨£å°æ¯”åœ–å·²ä¿å­˜ç‚º 'bias_dataset_analysis.png'")
    plt.show()
    
    # =================================================================
    # 5. åœ¨ä¿®å¾©å¾Œè³‡æ–™ä¸Šé‡æ–°è¨“ç·´æ¨¡å‹
    # =================================================================
    print("\nğŸ”„ ç¬¬äº”æ­¥: é‡æ–°è¨“ç·´æ¨¡å‹")
    print("-" * 50)
    print("æ­£åœ¨ä½¿ç”¨ä¿®å¾©å¾Œçš„è³‡æ–™é‡æ–°è¨“ç·´æ¨¡å‹...")
    
    # é‡æ–°è¨“ç·´æ¨¡å‹
    print("   ğŸ”„ è¨“ç·´ä¿®å¾©å¾Œçš„æ¨¡å‹...")
    model_fixed = LogisticRegression(random_state=42, max_iter=1000)
    model_fixed.fit(X_train_resampled, y_train_resampled)
    
    # è©•ä¼°ä¿®å¾©å¾Œçš„æ¨¡å‹
    print("   ğŸ“ˆ è©•ä¼°ä¿®å¾©å¾Œçš„æ¨¡å‹...")
    y_pred_fixed = model_fixed.predict(X_test)
    
    # è¨ˆç®—ä¿®å¾©å¾Œæ¨¡å‹çš„æŒ‡æ¨™
    accuracy_fixed = accuracy_score(y_test, y_pred_fixed)
    f1_fixed = f1_score(y_test, y_pred_fixed)
    
    print(f"\n   ğŸ“Š ä¿®å¾©å¾Œæ¨¡å‹è¡¨ç¾:")
    print(f"      - æº–ç¢ºç‡: {accuracy_fixed:.4f}")
    print(f"      - F1åˆ†æ•¸: {f1_fixed:.4f}")
    
    # è©³ç´°åˆ†é¡å ±å‘Š
    print(f"\n   ğŸ“‹ è©³ç´°åˆ†é¡å ±å‘Š:")
    print(classification_report(y_test, y_pred_fixed, target_names=['éé•ç´„', 'é•ç´„']))
    
    # åˆ†æä¿®å¾©æ•ˆæœ
    print(f"\n   âœ… ä¿®å¾©æ•ˆæœåˆ†æ:")
    print(f"      - æº–ç¢ºç‡è®ŠåŒ–: {accuracy_biased:.4f} â†’ {accuracy_fixed:.4f} ({accuracy_fixed-accuracy_biased:+.4f})")
    print(f"      - F1åˆ†æ•¸è®ŠåŒ–: {f1_biased:.4f} â†’ {f1_fixed:.4f} ({f1_fixed-f1_biased:+.4f})")
    
    if f1_fixed > f1_biased:
        print(f"      - âœ… F1åˆ†æ•¸æå‡ {((f1_fixed-f1_biased)/f1_biased)*100:.1f}%ï¼Œä¿®å¾©æˆåŠŸ!")
    else:
        print(f"      - âš ï¸  F1åˆ†æ•¸è®ŠåŒ–æœ‰é™ï¼Œå¯èƒ½éœ€è¦å…¶ä»–ä¿®å¾©ç­–ç•¥")
    
    # =================================================================
    # 6. æ¨¡å‹å°æ¯”åˆ†æ
    # =================================================================
    print("\nğŸ“Š ç¬¬å…­æ­¥: æ¨¡å‹å°æ¯”åˆ†æ")
    print("-" * 50)
    print("æ­£åœ¨æ¯”è¼ƒåè¦‹æ¨¡å‹å’Œä¿®å¾©å¾Œæ¨¡å‹çš„è¡¨ç¾...")
    
    # å‰µå»ºæ··æ·†çŸ©é™£å°æ¯”åœ–
    print("   ğŸ“ˆ å‰µå»ºæ··æ·†çŸ©é™£å°æ¯”åœ–...")
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
    
    # åè¦‹æ¨¡å‹çš„æ··æ·†çŸ©é™£
    cm_biased = confusion_matrix(y_test, y_pred_biased)
    sns.heatmap(cm_biased, annot=True, fmt='d', cmap='Blues', ax=ax1)
    ax1.set_title('åè¦‹æ¨¡å‹æ··æ·†çŸ©é™£\n(å°å°‘æ•¸é¡åˆ¥è­˜åˆ¥èƒ½åŠ›å·®)')
    ax1.set_xlabel('é æ¸¬é¡åˆ¥')
    ax1.set_ylabel('å¯¦éš›é¡åˆ¥')
    ax1.set_xticklabels(['éé•ç´„', 'é•ç´„'])
    ax1.set_yticklabels(['éé•ç´„', 'é•ç´„'])
    
    # ä¿®å¾©å¾Œæ¨¡å‹çš„æ··æ·†çŸ©é™£
    cm_fixed = confusion_matrix(y_test, y_pred_fixed)
    sns.heatmap(cm_fixed, annot=True, fmt='d', cmap='Greens', ax=ax2)
    ax2.set_title('ä¿®å¾©å¾Œæ¨¡å‹æ··æ·†çŸ©é™£\n(é¡åˆ¥å¹³è¡¡è­˜åˆ¥èƒ½åŠ›)')
    ax2.set_xlabel('é æ¸¬é¡åˆ¥')
    ax2.set_ylabel('å¯¦éš›é¡åˆ¥')
    ax2.set_xticklabels(['éé•ç´„', 'é•ç´„'])
    ax2.set_yticklabels(['éé•ç´„', 'é•ç´„'])
    
    plt.tight_layout()
    plt.savefig('bias_model_comparison.png', dpi=300, bbox_inches='tight')
    print("   âœ… æ··æ·†çŸ©é™£å°æ¯”åœ–å·²ä¿å­˜ç‚º 'bias_model_comparison.png'")
    plt.show()
    
    # =================================================================
    # 7. ç¶œåˆåˆ†æç¸½çµ
    # =================================================================
    print("\n" + "=" * 80)
    print("ğŸ¯ åè¦‹åˆ†æèˆ‡ä¿®å¾©ç¸½çµ")
    print("=" * 80)
    
    print("\nğŸ“Š åè¦‹ä¾†æºåˆ†æ:")
    print("   ğŸ” æª¢æ¸¬åˆ°çš„åè¦‹é¡å‹:")
    print("      - æ¡æ¨£åè¦‹: è³‡æ–™é›†ä¸­é¡åˆ¥æ¯”ä¾‹åš´é‡ä¸å¹³è¡¡ (19:1)")
    print("      - ä»£è¡¨æ€§åè¦‹: å°‘æ•¸é¡åˆ¥æ¨£æœ¬ä¸è¶³")
    print("      - æ¸¬é‡åè¦‹: æ¨¡å‹å°å°‘æ•¸é¡åˆ¥çš„è­˜åˆ¥èƒ½åŠ›å·®")
    print("   âš ï¸  åè¦‹åš´é‡ç¨‹åº¦: åš´é‡ (æ¯”ä¾‹ > 10:1)")
    
    print("\nğŸ”§ è§£æ±ºæ–¹æ¡ˆæ•ˆæœ:")
    print("   âœ… æ¡ç”¨çš„ä¿®å¾©æŠ€è¡“:")
    print("      - SMOTEé‡æ¡æ¨£: å¹³è¡¡é¡åˆ¥åˆ†ä½ˆ")
    print("      - åˆæˆæ¨£æœ¬ç”Ÿæˆ: å¢åŠ å°‘æ•¸é¡åˆ¥ä»£è¡¨æ€§")
    print("      - æ¨¡å‹é‡æ–°è¨“ç·´: æå‡å…¬å¹³æ€§")
    
    print(f"   ğŸ“ˆ ä¿®å¾©æ•ˆæœé‡åŒ–:")
    print(f"      - æº–ç¢ºç‡: {accuracy_biased:.4f} â†’ {accuracy_fixed:.4f}")
    print(f"      - F1åˆ†æ•¸: {f1_biased:.4f} â†’ {f1_fixed:.4f}")
    print(f"      - æ”¹å–„ç¨‹åº¦: {((f1_fixed-f1_biased)/f1_biased)*100:.1f}%")
    
    if f1_fixed > f1_biased:
        print("   âœ… ä¿®å¾©æˆåŠŸ! æ¨¡å‹å…¬å¹³æ€§é¡¯è‘—æå‡")
    else:
        print("   âš ï¸  ä¿®å¾©æ•ˆæœæœ‰é™ï¼Œå»ºè­°å˜—è©¦å…¶ä»–ç­–ç•¥")
    
    print("\nğŸ’¡ é€²ä¸€æ­¥æ”¹é€²å»ºè­°:")
    print("   ğŸš€ è³‡æ–™å±¤é¢:")
    print("      - æ”¶é›†æ›´å¤šæ¨£åŒ–å’Œä»£è¡¨æ€§çš„è³‡æ–™")
    print("      - ä½¿ç”¨å¤šç¨®é‡æ¡æ¨£æŠ€è¡“çµ„åˆ")
    print("      - è€ƒæ…®è³‡æ–™å¢å¼·å’ŒåˆæˆæŠ€è¡“")
    
    print("   ğŸ¤– æ¼”ç®—æ³•å±¤é¢:")
    print("      - ä½¿ç”¨å…¬å¹³æ€§ç´„æŸçš„æå¤±å‡½æ•¸")
    print("      - è€ƒæ…®å°æŠ—æ€§å»åè¦‹æŠ€è¡“")
    print("      - å¯¦æ–½å¾Œè™•ç†å…¬å¹³æ€§èª¿æ•´")
    
    print("   ğŸ“Š ç›£æ§å±¤é¢:")
    print("      - å»ºç«‹æŒçºŒçš„åè¦‹ç›£æ§ç³»çµ±")
    print("      - å®šæœŸè©•ä¼°æ¨¡å‹åœ¨ä¸åŒç¾¤é«”ä¸Šçš„è¡¨ç¾")
    print("      - å¯¦æ–½A/Bæ¸¬è©¦é©—è­‰å…¬å¹³æ€§")
    
    print("\nğŸ‰ åè¦‹åˆ†æèˆ‡ä¿®å¾©æ¼”ç¤ºå®Œæˆ!")
    print("=" * 80)

if __name__ == "__main__":
    """
    Entry point for the Bias Dataset Analysis and Mitigation demonstration.
    
    This script can be run directly to see the complete bias detection and
    mitigation demonstration in action. It will show:
    - Bias detection and analysis techniques
    - Data-level bias mitigation strategies
    - Model performance comparison
    - Fairness evaluation and recommendations
    
    Run with: python openAI_fix_bias_dataset.py
    
    Requirements:
    - pandas >= 1.3.0
    - numpy >= 1.21.0
    - scikit-learn >= 1.0.0
    - imbalanced-learn >= 0.8.0
    - matplotlib >= 3.5.0
    - seaborn >= 0.11.0
    """
    print("ğŸš€ å•Ÿå‹•æœ‰åè¦‹è³‡æ–™é›†åˆ†æèˆ‡ä¿®å¾©æ¼”ç¤º")
    print("=" * 80)
    
    try:
        main()
        print("\nâœ… æ¼”ç¤ºæˆåŠŸå®Œæˆ!")
        print("ğŸ’¡ æç¤º: åœ¨å¯¦éš›é …ç›®ä¸­ï¼Œå»ºè­°æ ¹æ“šå…·é«”å ´æ™¯é¸æ“‡åˆé©çš„åè¦‹ç·©è§£ç­–ç•¥")
        print("   ä¸¦é€²è¡Œå……åˆ†çš„æ¸¬è©¦é©—è­‰ä»¥ç¢ºä¿å…¬å¹³æ€§ã€‚")
    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºéç¨‹ä¸­å‡ºé”™: {e}")
        print("\nğŸ’¡ æ•…éšœæ’é™¤æç¤º:")
        print("   â€¢ ç¢ºä¿å·²å®‰è£æ‰€æœ‰å¿…éœ€çš„åŒ…")
        print("   â€¢ æª¢æŸ¥Pythonç‰ˆæœ¬å…¼å®¹æ€§")
        print("   â€¢ æŸ¥çœ‹è©³ç´°éŒ¯èª¤ä¿¡æ¯é€²è¡Œèª¿è©¦")
        import traceback
        traceback.print_exc()