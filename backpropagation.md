

ğŸ“˜ åå‘ä¼ æ’­ï¼ˆBackpropagationï¼‰çš„æ•°å­¦åŸç†ä¸å®ç°

ğŸ§® æ•°å­¦åŸºç¡€

åå‘ä¼ æ’­çš„æ ¸å¿ƒæ˜¯ é“¾å¼æ³•åˆ™ (Chain Rule)ã€‚
ç¥ç»ç½‘ç»œä¸­çš„æ¯ä¸€å±‚å¯ä»¥çœ‹ä½œä¸€ä¸ªå‡½æ•°å˜æ¢ï¼š

a^{(l)} = f^{(l)}(z^{(l)}), \quad z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}

å…¶ä¸­ï¼š

ï¼šç¬¬  å±‚çš„è¾“å‡ºï¼ˆæ¿€æ´»å€¼ï¼‰

, ï¼šæƒé‡å’Œåç½®

ï¼šæ¿€æ´»å‡½æ•°


æŸå¤±å‡½æ•°ï¼ˆä»¥å‡æ–¹è¯¯å·® MSE ä¸ºä¾‹ï¼‰ï¼š

L = \frac{1}{2} \sum_i (y_i - \hat{y}_i)^2

ç›®æ ‡ï¼šé€šè¿‡æ¢¯åº¦ä¸‹é™æ›´æ–°å‚æ•°ï¼š

W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}, \quad 
b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}

ğŸ”‘ é“¾å¼æ³•åˆ™

æ¢¯åº¦è®¡ç®—ä¾èµ–é“¾å¼æ³•åˆ™ï¼š

\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}

\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}} = \big(W^{(l+1)}\big)^T \delta^{(l+1)} \odot f'(z^{(l)})

å…¶ä¸­  ç§°ä¸º è¯¯å·®é¡¹ã€‚


---

ğŸ“ Python å®ç°ï¼ˆçº¯ NumPy æ‰‹å†™ï¼‰

import numpy as np

# æ¿€æ´»å‡½æ•° (sigmoid) åŠå…¶å¯¼æ•°
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def sigmoid_derivative(x):
    return sigmoid(x) * (1 - sigmoid(x))

# åˆå§‹åŒ–å‚æ•°
np.random.seed(42)
X = np.array([[0,0],[0,1],[1,0],[1,1]])   # è¾“å…¥
y = np.array([[0],[1],[1],[0]])           # æœŸæœ›è¾“å‡º (XOR)

# ç½‘ç»œç»“æ„: 2 -> 2 -> 1
W1 = np.random.randn(2, 2)
b1 = np.zeros((1, 2))
W2 = np.random.randn(2, 1)
b2 = np.zeros((1, 1))

lr = 0.1   # å­¦ä¹ ç‡

# è®­ç»ƒå¾ªç¯
for epoch in range(10000):
    # å‰å‘ä¼ æ’­
    z1 = X @ W1 + b1
    a1 = sigmoid(z1)
    z2 = a1 @ W2 + b2
    a2 = sigmoid(z2)

    # æŸå¤± (MSE)
    loss = np.mean((y - a2) ** 2)

    # åå‘ä¼ æ’­
    d_a2 = -(y - a2)                         # dL/da2
    d_z2 = d_a2 * sigmoid_derivative(z2)     # dL/dz2
    dW2 = a1.T @ d_z2
    db2 = np.sum(d_z2, axis=0, keepdims=True)

    d_a1 = d_z2 @ W2.T
    d_z1 = d_a1 * sigmoid_derivative(z1)
    dW1 = X.T @ d_z1
    db1 = np.sum(d_z1, axis=0, keepdims=True)

    # å‚æ•°æ›´æ–°
    W1 -= lr * dW1
    b1 -= lr * db1
    W2 -= lr * dW2
    b2 -= lr * db2

    # æ¯1000æ¬¡æ‰“å°ä¸€æ¬¡æŸå¤±
    if epoch % 1000 == 0:
        print(f"Epoch {epoch}, Loss: {loss:.4f}")

# è®­ç»ƒç»“æŸï¼Œè¾“å‡ºé¢„æµ‹ç»“æœ
print("é¢„æµ‹ç»“æœï¼š")
print(a2.round(3))


---

âœ… è¾“å‡ºç¤ºä¾‹

è®­ç»ƒè‹¥å¹²è½®åï¼Œæ¨¡å‹èƒ½å¤Ÿé€¼è¿‘ XOR é€»è¾‘ï¼š

Epoch 0, Loss: 0.3330
Epoch 1000, Loss: 0.2486
Epoch 2000, Loss: 0.2447
...
Epoch 9000, Loss: 0.0098

é¢„æµ‹ç»“æœï¼š
[[0.021]
 [0.981]
 [0.981]
 [0.018]]


---

è¿™æ ·å†™åœ¨ GitHub ä¸Šçš„ Markdown æ–‡æ¡£ä¸­ï¼Œå…¬å¼ï¼ˆLaTeXï¼‰ã€Python ä»£ç å’Œè¾“å‡ºéƒ½èƒ½æ­£ç¡®æ¸²æŸ“ã€‚

è¦ä¸è¦æˆ‘å¸®ä½ æŠŠ Question 4: Optimize a slow machine learning pipeline ä¹Ÿæ•´ç†æˆ GitHub æ–‡æ¡£é£æ ¼çš„å®Œæ•´è¯´æ˜ + ä»£ç ç¤ºä¾‹ï¼Ÿ

