#!/usr/bin/env python3
"""
Transformer Attention Mechanism Implementation

This file contains the complete implementation of Transformer's attention mechanism
as described in transformer.md, including both PyTorch and NumPy versions.

Author: Generated from transformer.md
"""

import numpy as np
import math

# Try to import PyTorch, but don't fail if it's not available
try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    PYTORCH_AVAILABLE = True
except ImportError:
    print("Warning: PyTorch not available. Only NumPy implementation will work.")
    PYTORCH_AVAILABLE = False


# =============================================================================
# PyTorch Implementation
# =============================================================================

class ScaledDotProductAttention(nn.Module):
    """
    å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶
    
    è¿™æ˜¯ Transformer çš„æ ¸å¿ƒç»„ä»¶ï¼Œè®¡ç®—åºåˆ—ä¸­æ¯ä¸ªä½ç½®å¯¹å…¶ä»–ä½ç½®çš„æ³¨æ„åŠ›æƒé‡
    """
    def __init__(self, dropout_rate=0.1):
        super().__init__()
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, q, k, v, mask=None):
        """
        å‰å‘ä¼ æ’­è®¡ç®—æ³¨æ„åŠ›
        
        Args:
            q: Queries, å½¢çŠ¶ [batch_size, n_heads, seq_len, d_k]
            k: Keys, å½¢çŠ¶ [batch_size, n_heads, seq_len, d_k]
            v: Values, å½¢çŠ¶ [batch_size, n_heads, seq_len, d_v]
            mask: æ©ç , å½¢çŠ¶ [batch_size, 1, 1, seq_len] æˆ– [batch_size, 1, seq_len, seq_len]
        
        Returns:
            output: æ³¨æ„åŠ›è¾“å‡º, å½¢çŠ¶ [batch_size, n_heads, seq_len, d_v]
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ [batch_size, n_heads, seq_len, seq_len]
        """
        d_k = k.size(-1)  # è·å– key çš„ç»´åº¦
        
        # 1. è®¡ç®— Q å’Œ K^T çš„ç‚¹ç§¯ (æ³¨æ„åŠ›åˆ†æ•°)
        # (batch, h, seq_len, d_k) @ (batch, h, d_k, seq_len) -> (batch, h, seq_len, seq_len)
        attention_scores = torch.matmul(q, k.transpose(-2, -1))

        # 2. ç¼©æ”¾ï¼šé™¤ä»¥ sqrt(d_k) é˜²æ­¢æ¢¯åº¦æ¶ˆå¤±
        attention_scores = attention_scores / math.sqrt(d_k)

        # 3. (å¯é€‰) åº”ç”¨æ©ç ï¼Œå°†ä¸éœ€è¦çš„ä½ç½®è®¾ä¸ºè´Ÿæ— ç©·
        if mask is not None:
            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)

        # 4. è®¡ç®— Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡ (æ¦‚ç‡åˆ†å¸ƒ)
        attention_weights = F.softmax(attention_scores, dim=-1)
        attention_weights = self.dropout(attention_weights)

        # 5. æƒé‡ä¸ V ç›¸ä¹˜å¾—åˆ°æœ€ç»ˆè¾“å‡º
        # (batch, h, seq_len, seq_len) @ (batch, h, seq_len, d_v) -> (batch, h, seq_len, d_v)
        output = torch.matmul(attention_weights, v)

        return output, attention_weights


class MultiHeadAttention(nn.Module):
    """
    å®ç°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    
    é€šè¿‡å¤šä¸ªå¹¶è¡Œçš„æ³¨æ„åŠ›å¤´æ¥æ•è·ä¸åŒç±»å‹çš„å…³ç³»
    """
    def __init__(self, d_model, n_heads, dropout_rate=0.1):
        """
        åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            d_model: æ¨¡å‹çš„æ€»ç»´åº¦ (embedding dimension)
            n_heads: æ³¨æ„åŠ›å¤´çš„æ•°é‡
            dropout_rate: Dropout æ¦‚ç‡
        """
        super().__init__()
        assert d_model % n_heads == 0, "d_model must be divisible by n_heads"

        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads  # æ¯ä¸ªå¤´çš„ç»´åº¦

        # å®šä¹‰çº¿æ€§æŠ•å½±å±‚ï¼šä¸ºæ¯ä¸ªå¤´åˆ›å»ºç‹¬ç«‹çš„ Q, K, V æŠ•å½±
        self.W_q = nn.Linear(d_model, d_model)  # Query æŠ•å½±
        self.W_k = nn.Linear(d_model, d_model)  # Key æŠ•å½±
        self.W_v = nn.Linear(d_model, d_model)  # Value æŠ•å½±
        self.W_o = nn.Linear(d_model, d_model)  # è¾“å‡ºæŠ•å½±

        self.attention = ScaledDotProductAttention(dropout_rate)

    def forward(self, q, k, v, mask=None):
        """
        å‰å‘ä¼ æ’­è®¡ç®—å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            q, k, v: è¾“å…¥å¼ é‡, å½¢çŠ¶ [batch_size, seq_len, d_model]
            mask: å¯é€‰çš„æ©ç å¼ é‡
        
        Returns:
            output: å¤šå¤´æ³¨æ„åŠ›è¾“å‡º, å½¢çŠ¶ [batch_size, seq_len, d_model]
            attention_weights: æ³¨æ„åŠ›æƒé‡, å½¢çŠ¶ [batch_size, n_heads, seq_len, seq_len]
        """
        batch_size = q.size(0)

        # 1. çº¿æ€§æŠ•å½±ï¼šå°†è¾“å…¥æŠ•å½±åˆ° Q, K, V ç©ºé—´
        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]
        q, k, v = self.W_q(q), self.W_k(k), self.W_v(v)

        # 2. æ‹†åˆ†æˆå¤šä¸ªå¤´ï¼šé‡å¡‘å¼ é‡ä»¥æ”¯æŒå¤šå¤´å¹¶è¡Œè®¡ç®—
        # [batch_size, seq_len, d_model] -> [batch_size, n_heads, seq_len, d_k]
        q = q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        k = k.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)
        v = v.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›ï¼šå¯¹æ¯ä¸ªå¤´å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›
        # output: [batch_size, n_heads, seq_len, d_k]
        # attention_weights: [batch_size, n_heads, seq_len, seq_len]
        output, attention_weights = self.attention(q, k, v, mask)

        # 4. æ‹¼æ¥å¤´ï¼šå°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥
        # [batch_size, n_heads, seq_len, d_k] -> [batch_size, seq_len, d_model]
        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)

        # 5. æœ€ç»ˆçš„çº¿æ€§å˜æ¢ï¼šé€šè¿‡è¾“å‡ºæŠ•å½±å±‚
        # [batch_size, seq_len, d_model] -> [batch_size, seq_len, d_model]
        output = self.W_o(output)

        return output, attention_weights


# =============================================================================
# NumPy Implementation
# =============================================================================

def softmax(x):
    """
    å®ç°æ•°å€¼ç¨³å®šçš„ softmax å‡½æ•°
    
    Args:
        x: è¾“å…¥æ•°ç»„
    
    Returns:
        softmax è¾“å‡º
    """
    # å‡å»æœ€å¤§å€¼é˜²æ­¢æ•°å€¼æº¢å‡º
    e_x = np.exp(x - np.max(x, axis=-1, keepdims=True))
    return e_x / np.sum(e_x, axis=-1, keepdims=True)


def scaled_dot_product_attention(q, k, v, mask=None):
    """
    å®ç°ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ› (NumPy ç‰ˆæœ¬)
    
    Args:
        q: Query çŸ©é˜µ
        k: Key çŸ©é˜µ
        v: Value çŸ©é˜µ
        mask: å¯é€‰çš„æ©ç 
    
    Returns:
        output: æ³¨æ„åŠ›è¾“å‡º
        attention_weights: æ³¨æ„åŠ›æƒé‡
    """
    d_k = q.shape[-1]
    
    # 1. è®¡ç®— Q @ K.T (æ³¨æ„åŠ›åˆ†æ•°)
    attention_scores = np.matmul(q, k.swapaxes(-2, -1))
    
    # 2. ç¼©æ”¾ï¼šé™¤ä»¥ sqrt(d_k)
    attention_scores = attention_scores / np.sqrt(d_k)
    
    # 3. åº”ç”¨æ©ç  (å¦‚æœæä¾›)
    if mask is not None:
        attention_scores += (mask * -1e9)
    
    # 4. è®¡ç®— Softmax å¾—åˆ°æ³¨æ„åŠ›æƒé‡
    attention_weights = softmax(attention_scores)
    
    # 5. è®¡ç®—æœ€ç»ˆè¾“å‡º
    output = np.matmul(attention_weights, v)
    
    return output, attention_weights


class MultiHeadAttentionNumpy:
    """
    ä½¿ç”¨ NumPy å®ç°çš„å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    
    è¿™ä¸ªå®ç°å±•ç¤ºäº†åº•å±‚æ•°å­¦è¿ç®—ï¼Œä¸ä¾èµ–æ·±åº¦å­¦ä¹ æ¡†æ¶
    """
    def __init__(self, d_model, n_heads):
        """
        åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            d_model: æ¨¡å‹ç»´åº¦
            n_heads: æ³¨æ„åŠ›å¤´æ•°
        """
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_k = d_model // n_heads

        # åˆå§‹åŒ–æƒé‡çŸ©é˜µ (åœ¨å®é™…è®­ç»ƒä¸­ï¼Œè¿™äº›æƒé‡ä¼šè¢«å­¦ä¹ )
        # ä½¿ç”¨ Xavier åˆå§‹åŒ–
        self.W_q = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        self.W_k = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        self.W_v = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)
        self.W_o = np.random.randn(d_model, d_model) * np.sqrt(2.0 / d_model)

    def forward(self, q, k, v, mask=None):
        """
        å‰å‘ä¼ æ’­è®¡ç®—å¤šå¤´æ³¨æ„åŠ›
        
        Args:
            q, k, v: è¾“å…¥çŸ©é˜µ, å½¢çŠ¶ [batch_size, seq_len, d_model]
            mask: å¯é€‰çš„æ©ç 
        
        Returns:
            output: å¤šå¤´æ³¨æ„åŠ›è¾“å‡º
            attn_weights: æ³¨æ„åŠ›æƒé‡
        """
        batch_size = q.shape[0]
        seq_len = q.shape[1]

        # 1. çº¿æ€§æŠ•å½±ï¼šå°†è¾“å…¥æŠ•å½±åˆ° Q, K, V ç©ºé—´
        q = np.dot(q, self.W_q)
        k = np.dot(k, self.W_k)
        v = np.dot(v, self.W_v)

        # 2. æ‹†åˆ†å¤´ï¼šé‡å¡‘å¼ é‡ä»¥æ”¯æŒå¤šå¤´å¹¶è¡Œè®¡ç®—
        q = q.reshape(batch_size, seq_len, self.n_heads, self.d_k).swapaxes(1, 2)
        k = k.reshape(batch_size, seq_len, self.n_heads, self.d_k).swapaxes(1, 2)
        v = v.reshape(batch_size, seq_len, self.n_heads, self.d_k).swapaxes(1, 2)

        # 3. è®¡ç®—æ³¨æ„åŠ›ï¼šå¯¹æ¯ä¸ªå¤´å¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›
        output, attn_weights = scaled_dot_product_attention(q, k, v, mask)

        # 4. æ‹¼æ¥å¤´ï¼šå°†æ‰€æœ‰å¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥
        output = output.swapaxes(1, 2).reshape(batch_size, seq_len, self.d_model)

        # 5. æœ€ç»ˆçº¿æ€§å˜æ¢ï¼šé€šè¿‡è¾“å‡ºæŠ•å½±å±‚
        output = np.dot(output, self.W_o)

        return output, attn_weights


# =============================================================================
# Demo Functions
# =============================================================================

def demo_pytorch_implementation():
    """æ¼”ç¤º PyTorch å®ç°"""
    if not PYTORCH_AVAILABLE:
        print("=" * 60)
        print("PyTorch Implementation Demo")
        print("=" * 60)
        print("âŒ PyTorch not available. Skipping PyTorch demo.")
        print("Please install PyTorch to run this demo: pip install torch")
        return None, None
    
    print("=" * 60)
    print("PyTorch Implementation Demo")
    print("=" * 60)
    
    # è®¾ç½®å‚æ•°
    d_model = 512      # æ¨¡å‹ç»´åº¦
    n_heads = 8        # æ³¨æ„åŠ›å¤´æ•°
    seq_len = 10       # åºåˆ—é•¿åº¦
    batch_size = 64    # æ‰¹æ¬¡å¤§å°

    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—
    mha = MultiHeadAttention(d_model, n_heads)

    # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
    q = torch.randn(batch_size, seq_len, d_model)  # Query
    k = torch.randn(batch_size, seq_len, d_model)  # Key
    v = torch.randn(batch_size, seq_len, d_model)  # Value

    print(f"Input shapes:")
    print(f"  Query: {q.shape}")
    print(f"  Key:   {k.shape}")
    print(f"  Value: {v.shape}")

    # å‰å‘ä¼ æ’­
    output, attn_weights = mha(q, k, v)

    # æ‰“å°å½¢çŠ¶ä¿¡æ¯
    print(f"\nOutput shapes:")
    print(f"  Output: {output.shape}")
    print(f"  Attention weights: {attn_weights.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, seq_len, d_model), f"Expected output shape {(batch_size, seq_len, d_model)}, got {output.shape}"
    assert attn_weights.shape == (batch_size, n_heads, seq_len, seq_len), f"Expected attention weights shape {(batch_size, n_heads, seq_len, seq_len)}, got {attn_weights.shape}"
    
    print("âœ… PyTorch implementation works correctly!")
    
    return output, attn_weights


def demo_numpy_implementation():
    """æ¼”ç¤º NumPy å®ç°"""
    print("\n" + "=" * 60)
    print("NumPy Implementation Demo")
    print("=" * 60)
    
    # è®¾ç½®å‚æ•°
    d_model_np = 512    # æ¨¡å‹ç»´åº¦
    n_heads_np = 8      # æ³¨æ„åŠ›å¤´æ•°
    seq_len_np = 10     # åºåˆ—é•¿åº¦
    batch_size_np = 64  # æ‰¹æ¬¡å¤§å°

    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›æ¨¡å—
    mha_np = MultiHeadAttentionNumpy(d_model_np, n_heads_np)

    # åˆ›å»ºéšæœºè¾“å…¥æ•°æ®
    q_np = np.random.randn(batch_size_np, seq_len_np, d_model_np)
    k_np = np.random.randn(batch_size_np, seq_len_np, d_model_np)
    v_np = np.random.randn(batch_size_np, seq_len_np, d_model_np)

    print(f"Input shapes:")
    print(f"  Query: {q_np.shape}")
    print(f"  Key:   {k_np.shape}")
    print(f"  Value: {v_np.shape}")

    # å‰å‘ä¼ æ’­
    output_np, attn_weights_np = mha_np.forward(q_np, k_np, v_np)

    # æ‰“å°å½¢çŠ¶ä¿¡æ¯
    print(f"\nOutput shapes:")
    print(f"  Output: {output_np.shape}")
    print(f"  Attention weights: {attn_weights_np.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert output_np.shape == (batch_size_np, seq_len_np, d_model_np), f"Expected output shape {(batch_size_np, seq_len_np, d_model_np)}, got {output_np.shape}"
    assert attn_weights_np.shape == (batch_size_np, n_heads_np, seq_len_np, seq_len_np), f"Expected attention weights shape {(batch_size_np, n_heads_np, seq_len_np, seq_len_np)}, got {attn_weights_np.shape}"
    
    print("âœ… NumPy implementation works correctly!")
    
    return output_np, attn_weights_np


def demo_attention_mechanism():
    """æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶çš„å·¥ä½œåŸç†"""
    print("\n" + "=" * 60)
    print("Attention Mechanism Analysis")
    print("=" * 60)
    
    if not PYTORCH_AVAILABLE:
        print("âŒ PyTorch not available. Skipping attention mechanism demo.")
        print("Please install PyTorch to run this demo: pip install torch")
        return
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥å±•ç¤ºæ³¨æ„åŠ›æƒé‡
    d_model = 64
    n_heads = 4
    seq_len = 5
    batch_size = 1

    mha = MultiHeadAttention(d_model, n_heads)
    
    # åˆ›å»ºè¾“å…¥ï¼ˆè®©ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªtokenç›¸ä¼¼ï¼‰
    q = torch.randn(batch_size, seq_len, d_model)
    k = q.clone()  # è‡ªæ³¨æ„åŠ›ï¼šQ = K = V
    v = q.clone()
    
    # è®©ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªtokenæ›´ç›¸ä¼¼
    q[0, -1] = q[0, 0] + 0.1 * torch.randn(d_model)
    k[0, -1] = k[0, 0] + 0.1 * torch.randn(d_model)
    v[0, -1] = v[0, 0] + 0.1 * torch.randn(d_model)

    output, attn_weights = mha(q, k, v)
    
    # åˆ†æç¬¬ä¸€ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡
    first_head_weights = attn_weights[0, 0].detach().numpy()
    
    print("Attention weights for the first head (first token attending to all tokens):")
    for i, weight in enumerate(first_head_weights[0]):
        print(f"  Token {i}: {weight:.4f}")
    
    print(f"\nSum of attention weights: {first_head_weights[0].sum():.4f}")
    print("âœ… Attention weights sum to 1 (as expected for softmax)")


def complexity_analysis():
    """å±•ç¤ºå¤æ‚åº¦åˆ†æ"""
    print("\n" + "=" * 60)
    print("Complexity Analysis")
    print("=" * 60)
    
    if not PYTORCH_AVAILABLE:
        print("âŒ PyTorch not available. Skipping complexity analysis demo.")
        print("Please install PyTorch to run this demo: pip install torch")
        return
    
    import time
    
    # æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦çš„æ€§èƒ½
    d_model = 512
    n_heads = 8
    batch_size = 32
    
    sequence_lengths = [10, 50, 100, 200]
    
    print("Performance test with different sequence lengths:")
    print(f"Model dimension: {d_model}, Heads: {n_heads}, Batch size: {batch_size}")
    print("-" * 60)
    
    for seq_len in sequence_lengths:
        mha = MultiHeadAttention(d_model, n_heads)
        q = torch.randn(batch_size, seq_len, d_model)
        k = torch.randn(batch_size, seq_len, d_model)
        v = torch.randn(batch_size, seq_len, d_model)
        
        # é¢„çƒ­
        _ = mha(q, k, v)
        
        # è®¡æ—¶
        start_time = time.time()
        for _ in range(10):  # è¿è¡Œ10æ¬¡å–å¹³å‡
            _ = mha(q, k, v)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 10
        print(f"Seq length {seq_len:3d}: {avg_time:.4f}s (O(nÂ²) complexity expected)")
    
    print("\nNote: Time complexity should scale quadratically with sequence length")


# =============================================================================
# Main Execution
# =============================================================================

if __name__ == "__main__":
    print("Transformer Attention Mechanism Implementation")
    print("This script demonstrates both PyTorch and NumPy implementations")
    print("of the Transformer's multi-head attention mechanism.\n")
    
    try:
        # è¿è¡Œ PyTorch æ¼”ç¤º
        demo_pytorch_implementation()
        
        # è¿è¡Œ NumPy æ¼”ç¤º
        demo_numpy_implementation()
        
        # æ¼”ç¤ºæ³¨æ„åŠ›æœºåˆ¶
        demo_attention_mechanism()
        
        # å¤æ‚åº¦åˆ†æ
        complexity_analysis()
        
        print("\n" + "=" * 60)
        print("All demonstrations completed successfully! ğŸ‰")
        print("=" * 60)
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()
