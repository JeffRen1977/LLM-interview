<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Transformer Encoder Data Flow</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f8f9fa;
        }
        .container {
            background: white;
            padding: 40px;
            border-radius: 10px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }
        h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            border-left: 4px solid #3498db;
            padding-left: 15px;
        }
        h3 {
            color: #2c3e50;
            margin-top: 25px;
        }
        pre {
            background-color: #f8f9fa;
            border: 1px solid #e9ecef;
            border-radius: 5px;
            padding: 20px;
            overflow-x: auto;
            font-family: 'Courier New', monospace;
            font-size: 12px;
            line-height: 1.4;
            margin: 20px 0;
        }
        code {
            background-color: #f1f3f4;
            padding: 2px 6px;
            border-radius: 3px;
            font-family: 'Courier New', monospace;
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: left;
        }
        th {
            background-color: #3498db;
            color: white;
        }
        tr:nth-child(even) {
            background-color: #f2f2f2;
        }
        .highlight {
            background-color: #fff3cd;
            padding: 15px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
        }
        .flowchart {
            background-color: #f8f9fa;
            border: 2px solid #dee2e6;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
            white-space: pre;
            overflow-x: auto;
        }
        .math {
            background-color: #e3f2fd;
            border: 1px solid #2196f3;
            border-radius: 5px;
            padding: 15px;
            margin: 15px 0;
            font-family: 'Courier New', monospace;
        }
        ul, ol {
            margin: 15px 0;
            padding-left: 30px;
        }
        li {
            margin: 8px 0;
        }
        .summary-box {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 25px;
            border-radius: 10px;
            margin: 30px 0;
        }
        .summary-box h3 {
            color: white;
            margin-top: 0;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Transformer Encoder Data Flow Diagram</h1>
        
        <div class="highlight">
            <strong>Overview:</strong> This document provides a comprehensive visualization of how data flows through a Transformer Encoder block, showing the step-by-step process from input to output.
        </div>

        <h2>Data Flow Architecture</h2>

        <h3>1. Input Processing Stage</h3>
        <div class="flowchart">
┌─────────────────────────────────────────────────────────────────┐
│                        INPUT STAGE                             │
├─────────────────────────────────────────────────────────────────┤
│  ┌─────────────────────┐    ┌─────────────────────┐           │
│  │   Input Embeddings  │    │  Position Encoding  │           │
│  │     (d_model)       │    │     (d_model)       │           │
│  └─────────┬───────────┘    └─────────┬───────────┘           │
│            │                          │                       │
│            └──────────┐    ┌──────────┘                       │
│                       │    │                                  │
│                       ▼    ▼                                  │
│              ┌─────────────────────┐                          │
│              │      ADDITION       │                          │
│              │   (Element-wise)    │                          │
│              └─────────┬───────────┘                          │
│                        │                                      │
│                        ▼                                      │
│              ┌─────────────────────┐                          │
│              │  Positional Embedding│                          │
│              │     Vector (d_model) │                          │
└──────────────┴─────────────────────┴──────────────────────────┘
        </div>

        <div class="highlight">
            <strong>What happens here:</strong>
            <ul>
                <li>Input tokens are converted to dense vectors (embeddings)</li>
                <li>Positional information is added to preserve sequence order</li>
                <li>The two vectors are combined through element-wise addition</li>
            </ul>
        </div>

        <h3>2. Multi-Head Self-Attention Sub-layer</h3>
        <div class="flowchart">
┌─────────────────────────────────────────────────────────────────┐
│              MULTI-HEAD ATTENTION SUB-LAYER                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                 RESIDUAL CONNECTION                     │   │
│  │              (Skip Connection Input)                   │   │
│  └─────────────────────┬───────────────────────────────────┘   │
│                        │                                     │
│                        ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              POSITIONAL EMBEDDING VECTOR                │   │
│  │                     (d_model)                          │   │
│  └─────────────────────┬───────────────────────────────────┘   │
│                        │                                     │
│                        ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              MULTI-HEAD ATTENTION MECHANISM            │   │
│  │  ┌─────────────┐ ┌─────────────┐ ┌─────────────┐      │   │
│  │  │   Head 1    │ │   Head 2    │ │   Head h    │      │   │
│  │  │ (d_k, d_v)  │ │ (d_k, d_v)  │ │ (d_k, d_v)  │      │   │
│  │  └─────┬───────┘ └─────┬───────┘ └─────┬───────┘      │   │
│  │        │                │               │               │   │
│  │        └────────────────┼───────────────┘               │   │
│  │                         │                               │   │
│  │                         ▼                               │   │
│  │              ┌─────────────────────┐                    │   │
│  │              │   CONCATENATE HEADS │                    │   │
│  │              │      (d_model)      │                    │   │
│  │              └─────────┬───────────┘                    │   │
│  │                        │                                │   │
│  │                        ▼                                │   │
│  │              ┌─────────────────────┐                    │   │
│  │              │   LINEAR PROJECTION │                    │   │
│  │              │      (d_model)      │                    │   │
│  └──────────────┴─────────┬───────────┴────────────────────┘   │
│                           │                                   │
│                           ▼                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              ATTENTION OUTPUT                          │   │
│  │                   (d_model)                           │   │
└──┴─────────────────────────────────────────────────────────┴───┘
        </div>

        <div class="highlight">
            <strong>Key Components:</strong>
            <ul>
                <li><strong>Query (Q):</strong> Represents what the token is looking for</li>
                <li><strong>Key (K):</strong> Represents what the token offers</li>
                <li><strong>Value (V):</strong> Represents the actual content</li>
                <li><strong>Attention Score:</strong> <code>Attention(Q,K,V) = softmax(QK^T/√d_k)V</code></li>
            </ul>
        </div>

        <h3>3. First Add & Layer Normalization</h3>
        <div class="flowchart">
┌─────────────────────────────────────────────────────────────────┐
│              FIRST ADD & LAYER NORMALIZATION                   │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────┐    ┌─────────────────────┐           │
│  │  RESIDUAL INPUT     │    │ ATTENTION OUTPUT    │           │
│  │   (d_model)         │    │     (d_model)       │           │
│  └─────────┬───────────┘    └─────────┬───────────┘           │
│            │                          │                       │
│            └──────────┐    ┌──────────┘                       │
│                       │    │                                  │
│                       ▼    ▼                                  │
│              ┌─────────────────────┐                          │
│              │      ADDITION       │                          │
│              │   (Element-wise)    │                          │
│              └─────────┬───────────┘                          │
│                        │                                      │
│                        ▼                                      │
│              ┌─────────────────────┐                          │
│              │   LAYER NORMALIZATION│                          │
│              │     (d_model)       │                          │
└──────────────┴─────────────────────┴──────────────────────────┘
        </div>

        <div class="highlight">
            <strong>What happens here:</strong>
            <ul>
                <li><strong>Residual Connection:</strong> Adds the original input to the attention output</li>
                <li><strong>Layer Normalization:</strong> Normalizes the sum to stabilize training</li>
                <li><strong>Formula:</strong> <code>LayerNorm(x + Sublayer(x))</code></li>
            </ul>
        </div>

        <h3>4. Feed-Forward Network Sub-layer</h3>
        <div class="flowchart">
┌─────────────────────────────────────────────────────────────────┐
│              FEED-FORWARD NETWORK SUB-LAYER                    │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │                 RESIDUAL CONNECTION                     │   │
│  │              (Skip Connection Input)                   │   │
│  └─────────────────────┬───────────────────────────────────┘   │
│                        │                                     │
│                        ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              NORMALIZED VECTOR                          │   │
│  │                   (d_model)                            │   │
│  └─────────────────────┬───────────────────────────────────┘   │
│                        │                                     │
│                        ▼                                     │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              FEED-FORWARD NETWORK                      │   │
│  │  ┌─────────────────────┐ ┌─────────────────────┐      │   │
│  │  │   LINEAR LAYER 1    │ │   LINEAR LAYER 2    │      │   │
│  │  │   (d_model → d_ff)  │ │   (d_ff → d_model)  │      │   │
│  │  └─────────┬───────────┘ └─────────┬───────────┘      │   │
│  │            │                        │                  │   │
│  │            └────────────┐  ┌────────┘                  │   │
│  │                         │  │                            │   │
│  │                         ▼  ▼                            │   │
│  │              ┌─────────────────────┐                    │   │
│  │              │   ACTIVATION (ReLU) │                    │   │
│  │              └─────────┬───────────┘                    │   │
│  └────────────────────────┼─────────────────────────────────┘   │
│                           │                                   │
│                           ▼                                   │
│  ┌─────────────────────────────────────────────────────────┐   │
│  │              FFN OUTPUT                                │   │
│  │                   (d_model)                           │   │
└──┴─────────────────────────────────────────────────────────┴───┘
        </div>

        <div class="highlight">
            <strong>FFN Architecture:</strong>
            <ul>
                <li><strong>First Linear Layer:</strong> Expands from <code>d_model</code> to <code>d_ff</code> (typically 4x larger)</li>
                <li><strong>ReLU Activation:</strong> Introduces non-linearity</li>
                <li><strong>Second Linear Layer:</strong> Contracts back to <code>d_model</code></li>
            </ul>
        </div>

        <h3>5. Second Add & Layer Normalization</h3>
        <div class="flowchart">
┌─────────────────────────────────────────────────────────────────┐
│              SECOND ADD & LAYER NORMALIZATION                  │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────┐    ┌─────────────────────┐           │
│  │  RESIDUAL INPUT     │    │    FFN OUTPUT       │           │
│  │   (d_model)         │    │     (d_model)       │           │
│  └─────────┬───────────┘    └─────────┬───────────┘           │
│            │                          │                       │
│            └──────────┐    ┌──────────┘                       │
│                       │    │                                  │
│                       ▼    ▼                                  │
│              ┌─────────────────────┐                          │
│              │      ADDITION       │                          │
│              │   (Element-wise)    │                          │
│              └─────────┬───────────┘                          │
│                        │                                      │
│                        ▼                                      │
│              ┌─────────────────────┐                          │
│              │   LAYER NORMALIZATION│                          │
│              │     (d_model)       │                          │
└──────────────┴─────────────────────┴──────────────────────────┘
        </div>

        <h3>6. Complete Transformer Encoder Block Flow</h3>
        <div class="flowchart">
┌─────────────────────────────────────────────────────────────────┐
│                    COMPLETE DATA FLOW                          │
├─────────────────────────────────────────────────────────────────┤
│                                                                 │
│  ┌─────────────────────┐                                        │
│  │   INPUT TOKENS      │                                        │
│  └─────────┬───────────┘                                        │
│            │                                                    │
│            ▼                                                    │
│  ┌─────────────────────┐                                        │
│  │  EMBEDDING + POS    │                                        │
│  │     (d_model)       │                                        │
│  └─────────┬───────────┘                                        │
│            │                                                    │
│            ▼                                                    │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │              SUB-LAYER 1: MULTI-HEAD ATTENTION         │    │
│  │  ┌─────────────────┐  ┌─────────────────┐              │    │
│  │  │   RESIDUAL      │  │   ATTENTION     │              │    │
│  │  │   CONNECTION    │  │   MECHANISM     │              │    │
│  │  └───────┬─────────┘  └───────┬─────────┘              │    │
│  │          │                    │                          │    │
│  │          └────────┐  ┌────────┘                          │    │
│  │                   │  │                                   │    │
│  │                   ▼  ▼                                   │    │
│  │          ┌─────────────────┐                              │    │
│  │          │   ADD + NORM    │                              │    │
│  │          └───────┬─────────┘                              │    │
│  └──────────────────┼────────────────────────────────────────┘    │
│                     │                                            │
│                     ▼                                            │
│  ┌─────────────────────────────────────────────────────────┐    │
│  │              SUB-LAYER 2: FEED-FORWARD NETWORK          │    │
│  │  ┌─────────────────┐  ┌─────────────────┐              │    │
│  │  │   RESIDUAL      │  │   FFN LAYERS    │              │    │
│  │  │   CONNECTION    │  │                 │              │    │
│  │  └───────┬─────────┘  └───────┬─────────┘              │    │
│  │          │                    │                          │    │
│  │          └────────┐  ┌────────┘                          │    │
│  │                   │  │                                   │    │
│  │                   ▼  ▼                                   │    │
│  │          ┌─────────────────┐                              │    │
│  │          │   ADD + NORM    │                              │    │
│  │          └───────┬─────────┘                              │    │
│  └──────────────────┼────────────────────────────────────────┘    │
│                     │                                            │
│                     ▼                                            │
│  ┌─────────────────────┐                                        │
│  │   ENCODER OUTPUT    │                                        │
│  │     (d_model)       │                                        │
│  └─────────────────────┘                                        │
└─────────────────────────────────────────────────────────────────┘
        </div>

        <h2>Mathematical Formulations</h2>

        <h3>1. Multi-Head Attention</h3>
        <div class="math">
MultiHead(Q,K,V) = Concat(head_1, ..., head_h)W^O
where head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)
        </div>

        <h3>2. Scaled Dot-Product Attention</h3>
        <div class="math">
Attention(Q,K,V) = softmax(QK^T/√d_k)V
        </div>

        <h3>3. Feed-Forward Network</h3>
        <div class="math">
FFN(x) = max(0, xW_1 + b_1)W_2 + b_2
        </div>

        <h3>4. Layer Normalization</h3>
        <div class="math">
LayerNorm(x) = γ * (x - μ)/√(σ² + ε) + β
        </div>

        <h2>Key Design Principles</h2>

        <h3>1. **Residual Connections**</h3>
        <ul>
            <li>Help with gradient flow during backpropagation</li>
            <li>Allow the model to learn residual functions</li>
            <li>Formula: <code>x + Sublayer(x)</code></li>
        </ul>

        <h3>2. **Layer Normalization**</h3>
        <ul>
            <li>Stabilizes training by normalizing activations</li>
            <li>Applied after each sub-layer</li>
            <li>Helps with training deep networks</li>
        </ul>

        <h3>3. **Multi-Head Attention**</h3>
        <ul>
            <li>Allows the model to attend to different positions simultaneously</li>
            <li>Each head learns different attention patterns</li>
            <li>Enables parallel processing</li>
        </ul>

        <h3>4. **Positional Encoding**</h3>
        <ul>
            <li>Provides sequence order information</li>
            <li>Uses sinusoidal functions for generalization</li>
            <li>Added to input embeddings</li>
        </ul>

        <h2>Data Dimensions Throughout the Flow</h2>

        <table>
            <thead>
                <tr>
                    <th>Stage</th>
                    <th>Dimension</th>
                    <th>Description</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td>Input Tokens</td>
                    <td><code>(seq_len,)</code></td>
                    <td>Sequence of token IDs</td>
                </tr>
                <tr>
                    <td>Embeddings</td>
                    <td><code>(seq_len, d_model)</code></td>
                    <td>Dense vector representations</td>
                </tr>
                <tr>
                    <td>Positional Encoding</td>
                    <td><code>(seq_len, d_model)</code></td>
                    <td>Position information</td>
                </tr>
                <tr>
                    <td>Combined Input</td>
                    <td><code>(seq_len, d_model)</code></td>
                    <td>Embeddings + Position</td>
                </tr>
                <tr>
                    <td>Query/Key/Value</td>
                    <td><code>(seq_len, d_k)</code> or <code>(seq_len, d_v)</code></td>
                    <td>Per-head dimensions</td>
                </tr>
                <tr>
                    <td>Attention Output</td>
                    <td><code>(seq_len, d_model)</code></td>
                    <td>After concatenation and projection</td>
                </tr>
                <tr>
                    <td>FFN Hidden</td>
                    <td><code>(seq_len, d_ff)</code></td>
                    <td>Expanded dimension (usually 4×d_model)</td>
                </tr>
                <tr>
                    <td>Final Output</td>
                    <td><code>(seq_len, d_model)</code></td>
                    <td>Ready for next encoder block</td>
                </tr>
            </tbody>
        </table>

        <h2>Training Considerations</h2>

        <h3>1. **Gradient Flow**</h3>
        <ul>
            <li>Residual connections help maintain gradient flow</li>
            <li>Layer normalization stabilizes training</li>
            <li>Proper initialization is crucial</li>
        </ul>

        <h3>2. **Computational Complexity**</h3>
        <ul>
            <li>Self-attention: O(n²) where n is sequence length</li>
            <li>FFN: O(n × d_model × d_ff)</li>
            <li>Memory usage scales with sequence length</li>
        </ul>

        <h3>3. **Hyperparameters**</h3>
        <ul>
            <li><code>d_model</code>: Model dimension (typically 512, 768, or 1024)</li>
            <li><code>d_ff</code>: Feed-forward dimension (typically 4×d_model)</li>
            <li><code>h</code>: Number of attention heads (typically 8 or 16)</li>
            <li><code>d_k</code>, <code>d_v</code>: Per-head dimensions (d_model/h)</li>
        </ul>

        <div class="summary-box">
            <h3>Summary</h3>
            <p>The Transformer Encoder processes data through a sophisticated pipeline:</p>
            <ol>
                <li><strong>Input Processing:</strong> Tokens → Embeddings + Positional Encoding</li>
                <li><strong>Self-Attention:</strong> Captures relationships between all positions</li>
                <li><strong>Residual + Norm:</strong> Stabilizes training and maintains information flow</li>
                <li><strong>Feed-Forward:</strong> Adds non-linearity and capacity</li>
                <li><strong>Final Residual + Norm:</strong> Prepares output for next layer</li>
            </ol>
            
            <p>This architecture enables the model to:</p>
            <ul>
                <li>Process sequences in parallel (unlike RNNs)</li>
                <li>Capture long-range dependencies effectively</li>
                <li>Scale to very deep architectures</li>
                <li>Maintain stable training dynamics</li>
            </ul>
            
            <p>The key innovation lies in the self-attention mechanism, which allows each position to attend to all other positions, making it possible to capture complex relationships regardless of distance in the sequence.</p>
        </div>
    </div>
</body>
</html>

