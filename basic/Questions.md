## Problem #1: Transformer 注意力机制详解

好的，这是一个非常经典且重要的机器学习面试问题。它旨在考察你对 Transformer 核心组件的深入理解和代码实现能力。

下面我将为你提供详细的解释、从零开始的代码实现（PyTorch 和 NumPy 版本）以及复杂度分析，完全遵循面试题目的要求。

## 详细解释：Transformer 注意力机制
Transformer 的核心是 自注意力机制（Self-Attention），它允许模型在处理一个序列（如一个句子）时，为序列中的每个单词（token）都计算一个“注意力分数”，这个分数决定了在编码当前单词时，应该对序列中其他单词投入多少关注度。
这个过程主要涉及三个关键的角色，它们都是从同一个输入向量（词嵌入）通过不同的线性变换得到的：
 * Query (Q)：代表当前正在处理的单词，它会去“查询”序列中所有其他的单词。
 * Key (K)：代表序列中可以被“查询”的单词。Query 会和每一个 Key 计算相似度。
 * Value (V)：代表序列中单词的实际内容。计算出的注意力分数会作用在 Value 上，用来加权求和，得到最终的输出。
1. 缩放点积注意力 (Scaled Dot-Product Attention)
这是注意力机制最核心的计算部分。它的计算过程分为以下几步：
 * 计算相似度分数：将每个 Query 向量与所有 Key 向量进行点积运算。这可以高效地通过矩阵乘法 Q \\cdot K^T 实现。
 * 缩放 (Scaling)：为了防止点积结果过大导致梯度消失（尤其是在 softmax 函数中），需要将点积结果除以一个缩放因子。这个因子通常是 Key 向量维度 $d_k$ 的平方根，即 $\frac{1}{\sqrt{d_k}}$。
 * 可选的掩码 (Masking)：在某些场景下（例如，在解码器中防止看到未来的信息），需要应用一个掩码，将特定位置的分数设置为一个非常小的负数（如 -1e9），这样在经过 softmax 后，这些位置的权重会趋近于 0。
 * 计算注意力权重：对缩放后的分数应用 Softmax 函数，将其转换为概率分布，得到每个单词相对于当前单词的注意力权重。
 * 加权求和：将计算出的注意力权重矩阵与 Value 矩阵相乘，得到最终的加权输出。
其数学公式为：

$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V$$
2. 多头注意力 (Multi-Head Attention)
为了让模型能够从不同的表示子空间中共同学习信息，Transformer 引入了“多头”机制。它不是只计算一次注意力，而是将 Query、Key 和 Value 通过不同的线性变换（权重矩阵）投影多次，然后并行地对每一次投影的结果执行缩放点积注意力。
其过程如下：
 * 线性投影：将输入的 Q, K, V 分别通过 h 个独立的线性层（$W_i^Q$, $W_i^K$, $W_i^V$）进行投影，得到 h 组不同的 Q, K, V。这里的 h 就是"头的数量"。
 * 并行计算注意力：对这 h 组 Q, K, V 分别执行缩放点积注意力计算，得到 h 个输出矩阵 $\text{head}_i$。
   $$\text{head}_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$$
 * 拼接 (Concatenate)：将这 h 个输出矩阵在特征维度上拼接起来。
 * 最终线性变换：将拼接后的矩阵通过最后一个线性层（$W^O$）进行变换，得到最终的多头注意力输出。
这种机制的好处在于，每个“头”可以学习到不同方面的注意力关系。例如，一个头可能关注语法关系，另一个头可能关注语义上的近义词关系。
## 代码实现

我们将严格遵循"从零实现"的要求，只使用基础的 torch.Tensor 或 numpy.array 操作。


## 复杂度分析

假设序列长度为 $n$，输入和输出的嵌入维度为 $d$。在多头注意力中，每个头的维度 $d_k = d / h$。

### 时间复杂度: $O(n^2 \cdot d)$
我们来分解计算瓶颈：

1. **初始线性投影**：将形状为 $(n, d)$ 的 Q, K, V 输入乘以形状为 $(d, d)$ 的权重矩阵。这个操作的复杂度是 $3 \times O(n \cdot d^2)$。

2. **计算注意力分数**：这是最关键的部分。我们计算 $Q \cdot K^T$。在拆分到多头后，Q 和 K 的形状变为 $(h, n, d_k)$。
   - $Q \cdot K^T$ 的计算是 $(n, d_k)$ 与 $(d_k, n)$ 的矩阵相乘，结果是 $(n, n)$。这个操作的复杂度是 $O(n^2 \cdot d_k)$。
   - 由于有 $h$ 个头，总复杂度是 $O(h \cdot n^2 \cdot d_k)$。
   - 因为 $h \cdot d_k = d$，所以这一步的复杂度是 $O(n^2 \cdot d)$。

3. **权重乘以V**：将形状为 $(n, n)$ 的注意力权重矩阵与形状为 $(n, d_v)$ 的 V 矩阵相乘（这里 $d_v=d_k$）。
   - 每个头的复杂度是 $O(n^2 \cdot d_v)$。
   - $h$ 个头的总复杂度是 $O(h \cdot n^2 \cdot d_v) = O(n^2 \cdot d)$。

4. **最终线性投影**：将拼接后的输出（形状为 $(n, d)$）乘以最终的权重矩阵（形状为 $(d, d)$），复杂度为 $O(n \cdot d^2)$。

**总结**：
总的时间复杂度为 $O(n \cdot d^2) + O(n^2 \cdot d)$。在典型的 Transformer 应用中，序列长度 $n$ 通常会大于嵌入维度 $d$（例如 $n=512, d=768$ 或者 $n=2048, d=1024$）。因此，主要的时间复杂度瓶颈是 $O(n^2 \cdot d)$。
### 空间复杂度: $O(n^2)$

1. **主要存储**：最大的中间产物是注意力权重矩阵，其形状为 $(h, n, n)$。因此，它占用的空间是 $O(h \cdot n^2)$。即使 $h$ 很大，它也是一个常数，所以通常简化为 $O(n^2)$。

2. **其他存储**：输入的 Q, K, V 以及输出的存储都是 $O(n \cdot d)$。

3. **模型参数**：权重矩阵 $W_q, W_k, W_v, W_o$ 的大小都是 $d \times d$，总共是 $4 \cdot d^2$，这是一个与序列长度 $n$ 无关的常数。

**总结**：
空间复杂度的瓶颈在于存储注意力分数矩阵，为 $O(n^2)$。这也是为什么标准 Transformer 难以处理非常长序列的原因。


## Problem 2: 反向传播（Backpropagation）的数学原理与实现

🧮 数学基础

反向传播的核心是 链式法则 (Chain Rule)。
神经网络中的每一层可以看作一个函数变换：

a^{(l)} = f^{(l)}(z^{(l)}), \quad z^{(l)} = W^{(l)} a^{(l-1)} + b^{(l)}

其中：

：第  层的输出（激活值）

, ：权重和偏置

：激活函数


损失函数（以均方误差 MSE 为例）：

L = \frac{1}{2} \sum_i (y_i - \hat{y}_i)^2

目标：通过梯度下降更新参数：

W^{(l)} \leftarrow W^{(l)} - \eta \frac{\partial L}{\partial W^{(l)}}, \quad 
b^{(l)} \leftarrow b^{(l)} - \eta \frac{\partial L}{\partial b^{(l)}}

🔑 链式法则

梯度计算依赖链式法则：

\frac{\partial L}{\partial W^{(l)}} = \frac{\partial L}{\partial a^{(l)}} \cdot \frac{\partial a^{(l)}}{\partial z^{(l)}} \cdot \frac{\partial z^{(l)}}{\partial W^{(l)}}

\delta^{(l)} = \frac{\partial L}{\partial z^{(l)}} = \big(W^{(l+1)}\big)^T \delta^{(l+1)} \odot f'(z^{(l)})

其中  称为 误差项。

✅ 输出示例

训练若干轮后，模型能够逼近 XOR 逻辑：

Epoch 0, Loss: 0.3330
Epoch 1000, Loss: 0.2486
Epoch 2000, Loss: 0.2447
...
Epoch 9000, Loss: 0.0098

预测结果：
[[0.021]
 [0.981]
 [0.981]
 [0.018]]


